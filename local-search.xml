<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/%E4%BD%BF%E7%94%A8gradio%E5%90%AF%E5%8A%A8web-ui%E6%97%B6%E5%87%BA%E7%8E%B0cannot%20import%20name%20&#39;RootModel&#39;%20from%20&#39;pydantic&#39;/"/>
    <url>/2024/09/23/%E4%BD%BF%E7%94%A8gradio%E5%90%AF%E5%8A%A8web-ui%E6%97%B6%E5%87%BA%E7%8E%B0cannot%20import%20name%20&#39;RootModel&#39;%20from%20&#39;pydantic&#39;/</url>
    
    <content type="html"><![CDATA[<h1 id="使用gradio启动web-ui时出现cannot-import-name-‘RootModel’-from-‘pydantic’"><a href="#使用gradio启动web-ui时出现cannot-import-name-‘RootModel’-from-‘pydantic’" class="headerlink" title="使用gradio启动web-ui时出现cannot import name ‘RootModel’ from ‘pydantic’"></a>使用gradio启动web-ui时出现cannot import name ‘RootModel’ from ‘pydantic’</h1><p>出现该报错的原因：pydantic版本与gradio版本不对应。</p><p>例：我使用的pydantic版本为1.10.14，报错时gradio的版本是最新版4.19.2。</p><p>找到gradio github源码中的requirements.txt：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">aiofiles</span>&gt;=<span class="hljs-number">22</span>.<span class="hljs-number">0</span>,&lt;<span class="hljs-number">24</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">altair</span>&gt;=<span class="hljs-number">4</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span>,&lt;<span class="hljs-number">6</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">fastapi</span><br><span class="hljs-attribute">ffmpy</span><br><span class="hljs-attribute">gradio_client</span>==<span class="hljs-number">0</span>.<span class="hljs-number">10</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">httpx</span>&gt;=<span class="hljs-number">0</span>.<span class="hljs-number">24</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">huggingface_hub</span>&gt;=<span class="hljs-number">0</span>.<span class="hljs-number">19</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">importlib_resources</span>&gt;=<span class="hljs-number">1</span>.<span class="hljs-number">3</span>,&lt;<span class="hljs-number">7</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">Jinja2</span>&lt;<span class="hljs-number">4</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">markupsafe</span>~=<span class="hljs-number">2</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">matplotlib</span>~=<span class="hljs-number">3</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">numpy</span>~=<span class="hljs-number">1</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">orjson</span>~=<span class="hljs-number">3</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">packaging</span><br><span class="hljs-attribute">pandas</span>&gt;=<span class="hljs-number">1</span>.<span class="hljs-number">0</span>,&lt;<span class="hljs-number">3</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">pillow</span>&gt;=<span class="hljs-number">8</span>.<span class="hljs-number">0</span>,&lt;<span class="hljs-number">11</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">pydantic</span>&gt;=<span class="hljs-number">2</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">python</span>-multipart&gt;=<span class="hljs-number">0</span>.<span class="hljs-number">0</span>.<span class="hljs-number">9</span>  # required for fastapi forms<br><span class="hljs-attribute">pydub</span><br><span class="hljs-attribute">pyyaml</span>&gt;=<span class="hljs-number">5</span>.<span class="hljs-number">0</span>,&lt;<span class="hljs-number">7</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">semantic_version</span>~=<span class="hljs-number">2</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">typing_extensions</span>~=<span class="hljs-number">4</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">uvicorn</span>&gt;=<span class="hljs-number">0</span>.<span class="hljs-number">14</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">typer</span>[<span class="hljs-literal">all</span>]&gt;=<span class="hljs-number">0</span>.<span class="hljs-number">9</span>,&lt;<span class="hljs-number">1</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">tomlkit</span>==<span class="hljs-number">0</span>.<span class="hljs-number">12</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">ruff</span>&gt;=<span class="hljs-number">0</span>.<span class="hljs-number">2</span>.<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><p>可以看到其要求的pydantic版本要大于等于2.0。</p><p>而我其他的包依赖的pydantic版本是1.x，因此我选择将gradio包的版本降低为3.48.0，问题解决。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/%E8%A7%A3%E5%86%B3debian%E6%9C%8D%E5%8A%A1%E5%99%A8DNS%E8%87%AA%E5%8A%A8%E6%B8%85%E9%99%A4%E9%97%AE%E9%A2%98/"/>
    <url>/2024/09/23/%E8%A7%A3%E5%86%B3debian%E6%9C%8D%E5%8A%A1%E5%99%A8DNS%E8%87%AA%E5%8A%A8%E6%B8%85%E9%99%A4%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1 id="解决Debian服务器使用NetworkManager出现的DNS自动清除问题"><a href="#解决Debian服务器使用NetworkManager出现的DNS自动清除问题" class="headerlink" title="解决Debian服务器使用NetworkManager出现的DNS自动清除问题"></a>解决Debian服务器使用NetworkManager出现的DNS自动清除问题</h1><p>使用vim编辑&#x2F;etc&#x2F;NetworkManager&#x2F;NetworkManager.conf中的内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo vim /etc/NetworkManager/NetworkManager.conf<br></code></pre></td></tr></table></figure><p>在该文件的[main]下方加上下面这一行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">dns=none<br></code></pre></td></tr></table></figure><p>然后重启NetworkManager服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo systemctl restart NetworkManager<br></code></pre></td></tr></table></figure><p>使用vim编辑&#x2F;etc&#x2F;resolv.conf:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo vim /etc/resolv.conf<br></code></pre></td></tr></table></figure><p>在该文件中添加nameserver：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nameserver 114.114.114.114<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/%E8%B6%85%E7%AE%97%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B1%B1%E6%B2%B3%E6%BA%90%E9%85%8D%E7%BD%AE/"/>
    <url>/2024/09/23/%E8%B6%85%E7%AE%97%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B1%B1%E6%B2%B3%E6%BA%90%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="超算服务器山河源配置"><a href="#超算服务器山河源配置" class="headerlink" title="超算服务器山河源配置"></a>超算服务器山河源配置</h1><h2 id="添加host"><a href="#添加host" class="headerlink" title="添加host"></a>添加host</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">vi /etc/hosts<br><span class="hljs-comment">### 追加一行 10.251.102.1 mirrors.shanhe.com</span><br>10.251.102.1 mirrors.shanhe.com<br><span class="hljs-comment">## 保存后退出 ：wq</span><br></code></pre></td></tr></table></figure><h2 id="ubuntu-18-04"><a href="#ubuntu-18-04" class="headerlink" title="ubuntu 18.04"></a>ubuntu 18.04</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> &gt; /etc/apt/sources.list &lt;&lt; <span class="hljs-string">&quot;EOF&quot;</span><br>deb https://mirrors.shanhe.com/ubuntu/ bionic main restricted universe multiverse<br>deb-src https://mirrors.shanhe.com/ubuntu/ bionic main restricted universe multiverse<br>deb https://mirrors.shanhe.com/ubuntu/ bionic-security main restricted universe multiverse<br>deb-src https://mirrors.shanhe.com/ubuntu/ bionic-security main restricted universe multiverse<br><br>deb https://mirrors.shanhe.com/ubuntu/ bionic-updates main restricted universe multiverse<br>deb-src https://mirrors.shanhe.com/ubuntu/ bionic-updates main restricted universe multiverse<br><br>deb https://mirrors.shanhe.com/ubuntu/ bionic-backports main restricted universe multiverse<br>deb-src https://mirrors.shanhe.com/ubuntu/ bionic-backports main restricted universe multiverse<br><br><span class="hljs-comment">## Not recommended</span><br><span class="hljs-comment"># deb http://mirrors.shanhe.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="hljs-comment"># deb-src http://mirrors.shanhe.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br>EOF<br></code></pre></td></tr></table></figure><h2 id="condarc"><a href="#condarc" class="headerlink" title=".condarc"></a>.condarc</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> &gt; ~/.condarc &lt;&lt; <span class="hljs-string">&quot;EOF&quot;</span><br>channels:<br>  - defaults<br>show_channel_urls: <span class="hljs-literal">true</span><br>default_channels:<br>  - https://mirrors.shanhe.com/anaconda/pkgs/main<br>  - https://mirrors.shanhe.com/anaconda/pkgs/r<br>  - https://mirrors.shanhe.com/anaconda/pkgs/msys2<br>custom_channels:<br>  bioconda: https://mirrors.shanhe.com/anaconda/cloud<br>  conda-forge: https://mirrors.shanhe.com/anaconda/cloud<br>ssl_verify: <span class="hljs-literal">false</span><br>EOF<br></code></pre></td></tr></table></figure><h2 id="pip"><a href="#pip" class="headerlink" title="pip"></a>pip</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 下面举例了pip安装numpy库两种方式</span><br><span class="hljs-comment">#方式1</span><br><span class="hljs-built_in">mkdir</span> ~/.pip<br><span class="hljs-built_in">cd</span> ~/.pip<br>vim pip.conf<br><br>[global]<br>index-url=https://mirrors.shanhe.com/simple<br>trusted-host = mirrors.shanhe.com<br><br>:wq<br>pip install nump<br><span class="hljs-comment">##方式2 </span><br>pip install nump -i https://mirrors.shanhe.com/simple<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/Ubuntu%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8apt-get%E5%AE%89%E8%A3%85%E5%8C%85%E6%97%B6%E5%87%BA%E7%8E%B0E%20Unable%20to%20locate%20package/"/>
    <url>/2024/09/23/Ubuntu%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8apt-get%E5%AE%89%E8%A3%85%E5%8C%85%E6%97%B6%E5%87%BA%E7%8E%B0E%20Unable%20to%20locate%20package/</url>
    
    <content type="html"><![CDATA[<h1 id="Ubuntu服务器使用apt-get安装包时出现E-Unable-to-locate-package解决方法"><a href="#Ubuntu服务器使用apt-get安装包时出现E-Unable-to-locate-package解决方法" class="headerlink" title="Ubuntu服务器使用apt-get安装包时出现E: Unable to locate package解决方法"></a>Ubuntu服务器使用apt-get安装包时出现E: Unable to locate package解决方法</h1><p>首先根据网络情况换源：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; /etc/apt/sources.list &lt;&lt; &quot;EOF&quot;<br>deb https://mirrors.shanhe.com/ubuntu/ bionic main restricted universe multiverse<br>deb-src https://mirrors.shanhe.com/ubuntu/ bionic main restricted universe multiverse<br>deb https://mirrors.shanhe.com/ubuntu/ bionic-security main restricted universe multiverse<br>deb-src https://mirrors.shanhe.com/ubuntu/ bionic-security main restricted universe multiverse<br><br>deb https://mirrors.shanhe.com/ubuntu/ bionic-updates main restricted universe multiverse<br>deb-src https://mirrors.shanhe.com/ubuntu/ bionic-updates main restricted universe multiverse<br><br>deb https://mirrors.shanhe.com/ubuntu/ bionic-backports main restricted universe multiverse<br>deb-src https://mirrors.shanhe.com/ubuntu/ bionic-backports main restricted universe multiverse<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># Not recommended</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">deb http://mirrors.shanhe.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">deb-src http://mirrors.shanhe.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br>EOF<br></code></pre></td></tr></table></figure><p>然后执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-get clean all<br>apt-get update<br></code></pre></td></tr></table></figure><p>然后再次使用apt-get安装对应的包即可解决问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-get install 包名<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/10-RAG%E5%AE%9E%E6%88%986-%E5%A6%82%E4%BD%95%E5%9C%A8LlamaIndex%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%87%AA%E5%B7%B1%E6%90%AD%E5%BB%BA%E7%9A%84API/"/>
    <url>/2024/09/23/10-RAG%E5%AE%9E%E6%88%986-%E5%A6%82%E4%BD%95%E5%9C%A8LlamaIndex%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%87%AA%E5%B7%B1%E6%90%AD%E5%BB%BA%E7%9A%84API/</url>
    
    <content type="html"><![CDATA[<h1 id="RAG实战6-如何在LlamaIndex使用自己搭建的大模型API"><a href="#RAG实战6-如何在LlamaIndex使用自己搭建的大模型API" class="headerlink" title="RAG实战6-如何在LlamaIndex使用自己搭建的大模型API"></a>RAG实战6-如何在LlamaIndex使用自己搭建的大模型API</h1><p>在<a href="https://www.cnblogs.com/yourenbo/p/18046379">搭建一个大模型API服务</a>中，我们介绍了如何使用SWIFT框架搭建一个大模型API服务。在RAG实战1-5中，我们一直使用的是本地加载大模型的方式来调用大模型，本文将介绍如何在LlamaIndex中使用自己搭建的大模型API。</p><p>LlamaIndex支持部分厂商的API配置，如OpenAI，但我们想使用的是自己在服务器上搭建的API服务，这个时候需要我们定制一个LLM类，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Any</span><br><span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> PromptTemplate, Settings, StorageContext, load_index_from_storage<br><span class="hljs-keyword">from</span> llama_index.core.base.llms.types <span class="hljs-keyword">import</span> LLMMetadata, CompletionResponse, CompletionResponseGen<br><span class="hljs-keyword">from</span> llama_index.core.llms <span class="hljs-keyword">import</span> CustomLLM<br><span class="hljs-keyword">from</span> llama_index.core.llms.callbacks <span class="hljs-keyword">import</span> llm_completion_callback<br><span class="hljs-keyword">from</span> llama_index.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbedding<br><span class="hljs-keyword">from</span> swift.llm <span class="hljs-keyword">import</span> get_model_list_client, XRequestConfig, inference_client<br><br><span class="hljs-comment"># API</span><br>model_list = get_model_list_client()<br>model_type = model_list.data[<span class="hljs-number">0</span>].<span class="hljs-built_in">id</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;API model_type: <span class="hljs-subst">&#123;model_type&#125;</span>&#x27;</span>)<br>request_config = XRequestConfig(seed=<span class="hljs-number">42</span>)<br><br><br><span class="hljs-comment"># 定制自己的LLM类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BianCangLLM</span>(<span class="hljs-title class_ inherited__">CustomLLM</span>):<br>    context_window: <span class="hljs-built_in">int</span> = <span class="hljs-number">4096</span><br>    num_output: <span class="hljs-built_in">int</span> = <span class="hljs-number">2048</span><br>    model_name: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;BianCang&quot;</span><br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">metadata</span>(<span class="hljs-params">self</span>) -&gt; LLMMetadata:<br>        <span class="hljs-string">&quot;&quot;&quot;Get LLM metadata.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> LLMMetadata(<br>            context_window=<span class="hljs-variable language_">self</span>.context_window,<br>            num_output=<span class="hljs-variable language_">self</span>.num_output,<br>            model_name=<span class="hljs-variable language_">self</span>.model_name,<br>        )<br><br><span class="hljs-meta">    @llm_completion_callback()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">complete</span>(<span class="hljs-params">self, prompt: <span class="hljs-built_in">str</span>, **kwargs: <span class="hljs-type">Any</span></span>) -&gt; CompletionResponse:<br>        resp = inference_client(model_type, prompt, [], request_config=request_config)<br>        <span class="hljs-keyword">return</span> CompletionResponse(text=resp.choices[<span class="hljs-number">0</span>].message.content)<br><br><span class="hljs-meta">    @llm_completion_callback()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">stream_complete</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, prompt: <span class="hljs-built_in">str</span>, **kwargs: <span class="hljs-type">Any</span></span><br><span class="hljs-params">    </span>) -&gt; CompletionResponseGen:<br>        resp = inference_client(model_type, prompt, [], request_config=request_config)<br>        response = <span class="hljs-string">&quot;&quot;</span><br>        <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> resp.choices[<span class="hljs-number">0</span>].message.content:<br>            response += token<br>            <span class="hljs-keyword">yield</span> CompletionResponse(text=response, delta=token)<br><br><br><span class="hljs-comment"># 定义system prompt</span><br>SYSTEM_PROMPT = <span class="hljs-string">&quot;&quot;&quot;你是一个医疗人工智能助手。&quot;&quot;&quot;</span><br>query_wrapper_prompt = PromptTemplate(<br>    <span class="hljs-string">&quot;[INST]&lt;&lt;SYS&gt;&gt;\n&quot;</span> + SYSTEM_PROMPT + <span class="hljs-string">&quot;&lt;&lt;/SYS&gt;&gt;\n\n&#123;query_str&#125;[/INST] &quot;</span><br>)<br><br><span class="hljs-comment"># 定义qa prompt</span><br>qa_prompt_tmpl_str = (<br>    <span class="hljs-string">&quot;上下文信息如下。\n&quot;</span><br>    <span class="hljs-string">&quot;---------------------\n&quot;</span><br>    <span class="hljs-string">&quot;&#123;context_str&#125;\n&quot;</span><br>    <span class="hljs-string">&quot;---------------------\n&quot;</span><br>    <span class="hljs-string">&quot;请根据上下文信息而不是先验知识来回答以下的查询。&quot;</span><br>    <span class="hljs-string">&quot;作为一个医疗人工智能助手，你的回答要尽可能严谨。\n&quot;</span><br>    <span class="hljs-string">&quot;Query: &#123;query_str&#125;\n&quot;</span><br>    <span class="hljs-string">&quot;Answer: &quot;</span><br>)<br>qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)<br><br><span class="hljs-comment"># 使用自定义的LLM API</span><br>Settings.llm = BianCangLLM()<br><br><span class="hljs-comment"># 使用llama-index-embeddings-huggingface构建本地embedding模型</span><br>Settings.embed_model = HuggingFaceEmbedding(<br>    model_name=<span class="hljs-string">&quot;E:\\LLMs\\bge-base-zh-v1.5&quot;</span><br>)<br><br><span class="hljs-comment"># 从存储文件中读取embedding向量和向量索引</span><br>storage_context = StorageContext.from_defaults(persist_dir=<span class="hljs-string">&quot;doc_emb&quot;</span>)<br>index = load_index_from_storage(storage_context)<br><br><span class="hljs-comment"># 构建查询引擎</span><br>query_engine = index.as_query_engine(similarity_top_k=<span class="hljs-number">5</span>)<br><br><span class="hljs-comment"># 更新查询引擎中的prompt template</span><br>query_engine.update_prompts(<br>    &#123;<span class="hljs-string">&quot;response_synthesizer:text_qa_template&quot;</span>: qa_prompt_tmpl&#125;<br>)<br><br><span class="hljs-comment"># 查询获得答案</span><br>response = query_engine.query(<span class="hljs-string">&quot;不耐疲劳，口燥、咽干可能是哪些证候？&quot;</span>)<br><span class="hljs-built_in">print</span>(response)<br><br></code></pre></td></tr></table></figure><p>代码的核心是实现BianCangLLM类，该类<strong>继承</strong>自LlamaIndex的CustomLLM类。我们需要<strong>重写</strong>父类中的<code>def metadata(self) -&gt; LLMMetadata</code>、<code>def complete(self, prompt: str, **kwargs: Any) -&gt; CompletionResponse</code>、<code>def stream_complete(self, prompt: str, **kwargs: Any) -&gt; CompletionResponseGen:</code>。其中，<code>metadata</code>负责定义大模型的一些参数属性；<code>complete</code>负责调用大模型API服务并直接返回响应；<code>stream_complete</code>负责调用大模型API服务并以流式输出的形式返回响应。</p><p>运行代码，同样可以得到类似于之前的效果：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs markdown">根据提供的上下文信息，口燥、咽干的症状可能与以下几个中医证候相关：<br><br><span class="hljs-bullet">1.</span> 津液不足证（4.6.1.1）：由于津液生成不足或体内燥热，可能导致口眼喉鼻干燥。<br><br><span class="hljs-bullet">2.</span> 津亏热结证（4.6.3.2）：津液亏虚加上热邪内结，也会出现口燥咽干的表现。<br><br><span class="hljs-bullet">3.</span> 津液亏涸证（4.6.1.2）：津液亏损严重时，口干、唇裂、鼻燥、舌燥是其特征，可能包括咽干。<br><br><span class="hljs-bullet">4.</span> 燥干清窍证（3.6.3.2）：气候干燥导致的津液耗损，会引起口鼻咽喉干燥。<br><br><span class="hljs-bullet">5.</span> 津伤化燥证（6.3.1）：燥热内蕴或内热化燥可能引起口干舌燥，伴有多尿、消瘦等症状。<br><br>因此，这些证候都可能与不耐疲劳和口燥咽干的临床表现相关，但具体诊断需要结合其他症状和中医辨证原则。建议患者就诊中医师以获取专业诊断。<br></code></pre></td></tr></table></figure><p>好处是，我们不需要每次启动RAG应用时都加载一遍大模型权重了。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/14-%E5%9C%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E9%83%A8%E7%BD%B2%E4%B8%AD%E5%8C%BB%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    <url>/2024/09/23/14-%E5%9C%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E9%83%A8%E7%BD%B2%E4%B8%AD%E5%8C%BB%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/</url>
    
    <content type="html"><![CDATA[<h1 id="在Linux服务器上部署中医知识图谱"><a href="#在Linux服务器上部署中医知识图谱" class="headerlink" title="在Linux服务器上部署中医知识图谱"></a>在Linux服务器上部署中医知识图谱</h1><h2 id="配置Java环境"><a href="#配置Java环境" class="headerlink" title="配置Java环境"></a>配置Java环境</h2><p>在Oracle官网或Open JDK官网下载JDK 17安装包，选择与操作系统及系统架构（32位或64位）相匹配的版本，此处使用Ubuntu系统进行演示。</p><p>使用以下命令解压JDK安装包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tar -xvf jdk-17_linux-x64_bin.tar.gz<br></code></pre></td></tr></table></figure><p>这将在当前目录下创建一个名为<code>jdk-17</code>的目录，并将JDK文件提取到其中。</p><p>使用以下命令编辑环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> JAVA_HOME=/yldm0226/KG/jdk-17<br><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$JAVA_HOME</span>/bin:<span class="hljs-variable">$PATH</span><br></code></pre></td></tr></table></figure><p>注意将JAVA_HOME替换为你解压JDK的实际路径。</p><p>使用以下命令刷新环境变量，使环境变量生效：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><p>验证是否安装成功：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">java -version<br></code></pre></td></tr></table></figure><p>如果安装成功，可以看到类似下面的输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">openjdk version <span class="hljs-string">&quot;17&quot;</span> 2021-09-14<br>OpenJDK Runtime Environment (build 17+35-2724)<br>OpenJDK 64-Bit Server VM (build 17+35-2724, mixed mode, sharing)<br></code></pre></td></tr></table></figure><h2 id="配置neo4j数据库"><a href="#配置neo4j数据库" class="headerlink" title="配置neo4j数据库"></a>配置neo4j数据库</h2><p>在neo4j官网下载neo4j社区版的安装包，选择与操作系统及系统架构（32位或64位）相匹配的版本，此处使用Ubuntu系统进行演示。</p><p>使用以下命令解压neo4j安装包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tar -xvf neo4j-community-5.17.0-unix.tar.gz<br></code></pre></td></tr></table></figure><p>使用以下命令编辑环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> NEO4J_HOME=/yldm0226/KG/neo4j-community-5.17.0<br><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$NEO4J_HOME</span>/bin:<span class="hljs-variable">$PATH</span><br></code></pre></td></tr></table></figure><p>使用以下命令刷新环境变量，使环境变量生效：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><p>如果想在命令行中运行neo4j，执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">neo4j console<br></code></pre></td></tr></table></figure><p>如果想在后台运行neo4j，执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">neo4j start<br></code></pre></td></tr></table></figure><p>启动成功后，可以看到以下输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">Directories <span class="hljs-keyword">in</span> use:<br>home:         /yldm0226/KG/neo4j-community-5.17.0<br>config:       /yldm0226/KG/neo4j-community-5.17.0/conf<br>logs:         /yldm0226/KG/neo4j-community-5.17.0/logs<br>plugins:      /yldm0226/KG/neo4j-community-5.17.0/plugins<br>import:       /yldm0226/KG/neo4j-community-5.17.0/import<br>data:         /yldm0226/KG/neo4j-community-5.17.0/data<br>certificates: /yldm0226/KG/neo4j-community-5.17.0/certificates<br>licenses:     /yldm0226/KG/neo4j-community-5.17.0/licenses<br>run:          /yldm0226/KG/neo4j-community-5.17.0/run<br>Starting Neo4j.<br>Started neo4j (pid:12498). It is available at http://localhost:7474<br>There may be a short delay <span class="hljs-keyword">until</span> the server is ready.<br></code></pre></td></tr></table></figure><h2 id="访问知识图谱"><a href="#访问知识图谱" class="headerlink" title="访问知识图谱"></a>访问知识图谱</h2><p>如果服务器有图形化界面，可以直接在浏览器中访问<a href="http://localhost:7474。">http://localhost:7474。</a></p><p>如果服务器没有图形化界面，需要借助ssh端口映射实现本地访问服务器上部署的知识图谱。</p><p>以Windows11系统为例，打开两个CMD窗口，分别输入以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh -L 7474:localhost:7474 -p ssh端口 用户名@ip<br><br>ssh -L 7687:localhost:7687 -p ssh端口 用户名@ip<br></code></pre></td></tr></table></figure><p>将ssh端口、用户名和ip替换为自己的。</p><p>以上两行命令将本地的7474端口和7687端口映射到了远程服务器的7474端口和7687端口，这样就可以本地访问服务器上部署的知识图谱了。</p><p>在本地浏览器中访问<a href="http://localhost:7474，可以看到neo4j的Web页面：">http://localhost:7474，可以看到neo4j的Web页面：</a></p><p><img src="/../images/neo4j.png"></p><p>在第一次访问<a href="http://localhost:7474时，默认的用户名和密码均为neo4j，在登录成功后，会要求我们修改一个密码，请记好这个密码，后面还要用到。">http://localhost:7474时，默认的用户名和密码均为neo4j，在登录成功后，会要求我们修改一个密码，请记好这个密码，后面还要用到。</a></p><h2 id="创建知识图谱"><a href="#创建知识图谱" class="headerlink" title="创建知识图谱"></a>创建知识图谱</h2><p>拉取开源中医知识图谱项目：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/ywjawmw/TCM_KG<br></code></pre></td></tr></table></figure><p>安装py2neo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install py2neo<br></code></pre></td></tr></table></figure><p>打开中医知识图谱项目中的Create_Graph.py，对代码做简单的修改并将连接数据库中的认证信息改为自己，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> py2neo <span class="hljs-keyword">import</span> Graph, Node, Relationship, NodeMatcher, RelationshipMatcher<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-comment"># 连接数据库</span><br>graph = Graph(<span class="hljs-string">&quot;http://localhost:7474&quot;</span>, auth=(<span class="hljs-string">&quot;neo4j&quot;</span>, <span class="hljs-string">&quot;你设置的neo4j的密码&quot;</span>),name=<span class="hljs-string">&quot;neo4j&quot;</span>)<br><br>matcher_node = NodeMatcher(graph)<br>matcher_relation = RelationshipMatcher(graph)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;baseline_all_kg_triples.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> file:<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tqdm(file.readlines()):<br>        entity_1, entity_2, relation = line.split(<span class="hljs-string">&quot;\t&quot;</span>)<br>        node_1 = matcher_node.<span class="hljs-keyword">match</span>(name=entity_1).first()<br>        <span class="hljs-keyword">if</span> node_1 <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            node_1 = Node(name=entity_1)<br>            graph.create(node_1)<br><br>        node_2 = matcher_node.<span class="hljs-keyword">match</span>(name=entity_2).first()<br>        <span class="hljs-keyword">if</span> node_2 <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            node_2 = Node(relation, name=entity_2)<br>            graph.create(node_2)<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> node_2.has_label(relation):<br>            node_2.add_label(relation)<br>            graph.push(node_2)<br><br>        r = Relationship(node_1, relation, node_2)<br>        graph.create(r)<br></code></pre></td></tr></table></figure><p>运行以上代码，就可以完成知识图谱的构建。</p><p>回到<a href="http://localhost:7474中，点击Database，就可以看到我们构建的知识图谱的节点和节点之间的关系类型：">http://localhost:7474中，点击Database，就可以看到我们构建的知识图谱的节点和节点之间的关系类型：</a></p><p><img src="/../images/neo4j-2.png"></p><p>我们可以运行命令或者点击左侧的标签查看知识图谱中的节点和关系：</p><p><img src="/../images/neo4j-3.png"></p><p>neo4j使用的是Cypher查询语言（是一种专门用于图数据库的查询语言），如果想进一步使用neo4j的命令查询，可以去学习Cypher查询语言。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/13-%E4%BD%BF%E7%94%A8Nginx%E5%B0%86%E5%A4%A7%E6%A8%A1%E5%9E%8BWeb%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2%E5%88%B0%E5%85%AC%E7%BD%91/"/>
    <url>/2024/09/23/13-%E4%BD%BF%E7%94%A8Nginx%E5%B0%86%E5%A4%A7%E6%A8%A1%E5%9E%8BWeb%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2%E5%88%B0%E5%85%AC%E7%BD%91/</url>
    
    <content type="html"><![CDATA[<h1 id="使用Nginx将大模型Web应用部署到公网"><a href="#使用Nginx将大模型Web应用部署到公网" class="headerlink" title="使用Nginx将大模型Web应用部署到公网"></a>使用Nginx将大模型Web应用部署到公网</h1><p>大模型训练完毕后，我们可以用SWIFT快速构建一个Web Demo大模型Web应用，本文将介绍如何使用Nginx将大模型Web应用部署到公网。</p><p>在进行后续步骤之前，先按照<a href="https://www.cnblogs.com/yourenbo/p/18046379">搭建一个大模型API服务</a>中的方法安装好SWIFT框架，并激活到你的conda环境。</p><h2 id="启动大模型Web应用"><a href="#启动大模型Web应用" class="headerlink" title="启动大模型Web应用"></a>启动大模型Web应用</h2><p>使用SWIFT提供的Web-UI启动大模型Web应用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">CUDA_VISIBLE_DEVICES=0 swift app-ui --model_type qwen1half-14b-chat --model_id_or_path /yldm0226/models/Qwen1.5-14B-Chat<br></code></pre></td></tr></table></figure><p>运行成功后，可以看到以下输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">Running on <span class="hljs-built_in">local</span> URL:  http://127.0.0.1:7860<br><br>To create a public <span class="hljs-built_in">link</span>, <span class="hljs-built_in">set</span> `share=True` <span class="hljs-keyword">in</span> `launch()`.<br></code></pre></td></tr></table></figure><p>此时，我们就可以通过<a href="http://127.0.0.1:7860访问该Web应用了。">http://127.0.0.1:7860访问该Web应用了。</a></p><p>如果我们想让其他人也能访问到这个网址，需要将Web应用部署到公网。</p><h2 id="配置Nginx服务器"><a href="#配置Nginx服务器" class="headerlink" title="配置Nginx服务器"></a>配置Nginx服务器</h2><p>这里我们使用Nginx来实现需求，Nginx是一个高性能的HTTP和反向代理web服务器，同时也提供了IMAP&#x2F;POP3&#x2F;SMTP服务。</p><p>以Ubuntu系统为例，使用以下命令安装Nginx：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt update<br><span class="hljs-built_in">sudo</span> apt install nginx<br></code></pre></td></tr></table></figure><p>找到Nginx的配置文件。通常情况下，在Ubuntu中，配置文件位于<code>/etc/nginx/nginx.conf</code>。使用文本编辑器打开配置文件。</p><p>在配置文件中找到<code>http</code>块，然后在其中添加一个新的<code>server</code>块。示例如下：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-section">server</span> &#123;<br>       <span class="hljs-attribute">listen</span> <span class="hljs-number">80</span>;<br>       <span class="hljs-attribute">server_name</span> 你的服务器ip地址;<br><br>       <span class="hljs-section">location</span> / &#123;<br>       <span class="hljs-attribute">proxy_pass</span> http://localhost:7860;<br>       <span class="hljs-attribute">proxy_http_version</span> <span class="hljs-number">1</span>.<span class="hljs-number">1</span>;<br>       <span class="hljs-attribute">proxy_set_header</span> Upgrade <span class="hljs-variable">$http_upgrade</span>;<br>       <span class="hljs-attribute">proxy_set_header</span> Connection <span class="hljs-string">&quot;Upgrade&quot;</span>;<br>       <span class="hljs-attribute">proxy_set_header</span> Host <span class="hljs-variable">$host</span>;<br>       <span class="hljs-attribute">proxy_set_header</span> X-Real-IP <span class="hljs-variable">$remote_addr</span>;<br>       <span class="hljs-attribute">proxy_set_header</span> X-Forwarded-For <span class="hljs-variable">$proxy_add_x_forwarded_for</span>;<br>       &#125;<br>   &#125;<br><br></code></pre></td></tr></table></figure><p>保存并关闭配置文件。</p><p>运行以下命令检查Nginx配置是否有语法错误：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> nginx -t<br></code></pre></td></tr></table></figure><p>如果没有错误，可以看到以下输出：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nginx">nginx: <span class="hljs-attribute">the</span> configuration file /etc/nginx/nginx.conf syntax is ok<br>nginx: configuration file /etc/nginx/nginx.conf test is successful<br></code></pre></td></tr></table></figure><p>如果有错误，请仔细检查并纠正错误。</p><p>运行以下命令启动nginx服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">service nginx start<br></code></pre></td></tr></table></figure><p>如果服务器上启用了防火墙（如iptables），需要确保将Nginx监听的80端口放行。</p><p>检查当前的iptables规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">iptables -L<br></code></pre></td></tr></table></figure><p>如果没有现有的规则允许80端口的流量通过，可以使用以下命令添加规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">iptables -A INPUT -p tcp --dport 80 -j ACCEPT<br></code></pre></td></tr></table></figure><p>这将允许TCP流量通过80端口。</p><p>如果服务器上运行着防火墙软件（如UFW），还需要确保它允许80端口的流量通过。可以使用以下命令启用UFW的80端口：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">ufw</span> <span class="hljs-literal">allow</span> <span class="hljs-number">80</span><br></code></pre></td></tr></table></figure><p>如果权限不足，需要在命令前加上<code>sudo</code>以获取管理员权限。</p><p>成功之后，就可以在本地浏览器中通过<a href="http://ip来访问服务器上的大模型Web应用了：">http://ip来访问服务器上的大模型Web应用了：</a></p><p><img src="C:\Users\WeiSibo\AppData\Roaming\Typora\typora-user-images\image-20240307213845498.png" alt="image-20240307213845498"></p><p>请注意，如果你的服务器网络是内网，本地需要额外的代理才能正常访问Web应用。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/12-%E7%9B%91%E6%8E%A7%E8%AE%AD%E7%BB%83/"/>
    <url>/2024/09/23/12-%E7%9B%91%E6%8E%A7%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<h1 id="监控大模型训练"><a href="#监控大模型训练" class="headerlink" title="监控大模型训练"></a>监控大模型训练</h1><p>大模型训练时间久，而且过程中容易出现各种各样的问题而中断，中断之后不及时续练的话对GPU资源是很大的浪费，但是我们又不能一直盯着程序。所以本文将介绍如何编写一个监控程序来监控大模型的训练，以方便我们在大模型训练出现异常时及时通知给我们。</p><p>监控的方式有很多，这里介绍两个方式。</p><h2 id="根据log文件大小变化监控训练是否进行"><a href="#根据log文件大小变化监控训练是否进行" class="headerlink" title="根据log文件大小变化监控训练是否进行"></a>根据log文件大小变化监控训练是否进行</h2><p>在<a href="">linux nohup指令详解</a>中，我们提到了使用Linux的nohup命令来运行训练脚本，该命令会创建一个日志文件，大模型在训练的过程中会不断输出内容，因此该日志文件的大小是随时在变化的。因此，我们可以通过隔一段时间判断该日志文件的大小是否变化来判断大模型的训练是否出现异常。如果大模型训练出现异常，我们需要通过一种常用的通讯方式来告知自己，这里选择使用邮箱（也可以使用短信、QQ&#x2F;微信通知等方式）。</p><p>核心代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> smtplib<br><span class="hljs-keyword">from</span> email.mime.text <span class="hljs-keyword">import</span> MIMEText<br><br><span class="hljs-comment"># 配置邮箱信息</span><br>SMTP_SERVER = <span class="hljs-string">&#x27;smtp.qq.com&#x27;</span><br>SMTP_PORT = <span class="hljs-number">587</span><br>EMAIL_USERNAME = <span class="hljs-string">&#x27;作为SMTP服务器的QQ邮箱&#x27;</span><br>EMAIL_PASSWORD = <span class="hljs-string">&#x27;你的QQ邮箱SMTP服务的密钥&#x27;</span><br>EMAIL_FROM = <span class="hljs-string">&#x27;作为SMTP服务器的QQ邮箱&#x27;</span><br>EMAIL_TO = <span class="hljs-string">&#x27;你接收通知邮件的QQ邮箱&#x27;</span><br>EMAIL_SUBJECT = <span class="hljs-string">&#x27;大模型训练终止提醒&#x27;</span><br><br><span class="hljs-comment"># 监测的文件路径</span><br>FILE_PATH = <span class="hljs-string">&#x27;/...../nohup.out&#x27;</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">send_email</span>(<span class="hljs-params">message</span>):<br>    msg = MIMEText(message)<br>    msg[<span class="hljs-string">&#x27;From&#x27;</span>] = EMAIL_FROM<br>    msg[<span class="hljs-string">&#x27;To&#x27;</span>] = EMAIL_TO<br>    msg[<span class="hljs-string">&#x27;Subject&#x27;</span>] = EMAIL_SUBJECT<br><br>    <span class="hljs-keyword">with</span> smtplib.SMTP(SMTP_SERVER, SMTP_PORT) <span class="hljs-keyword">as</span> server:<br>        server.starttls()<br>        server.login(EMAIL_USERNAME, EMAIL_PASSWORD)<br>        server.sendmail(from_addr=EMAIL_FROM, to_addrs=[EMAIL_TO], msg=msg.as_string())<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">monitor_file</span>():<br>    <span class="hljs-comment"># 获取初始文件大小</span><br>    initial_size = os.path.getsize(FILE_PATH)<br><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-comment"># 等待10分钟</span><br>        time.sleep(<span class="hljs-number">600</span>)<br><br>        <span class="hljs-comment"># 获取当前文件大小</span><br>        current_size = os.path.getsize(FILE_PATH)<br><br>        <span class="hljs-keyword">if</span> current_size == initial_size:<br>            <span class="hljs-comment"># 文件大小没有变化，发送警告邮件</span><br>            message = <span class="hljs-string">&#x27;日志文件在十分钟内没有发生变化，大模型训练可能已终止！&#x27;</span><br>            send_email(message)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 文件大小发生变化，更新初始文件大小</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;log changed: <span class="hljs-subst">&#123;current_size-initial_size&#125;</span>&#x27;</span>)<br>            initial_size = current_size<br>            <br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    monitor_file()<br></code></pre></td></tr></table></figure><h2 id="根据GPU显存占用率监控训练是否进行"><a href="#根据GPU显存占用率监控训练是否进行" class="headerlink" title="根据GPU显存占用率监控训练是否进行"></a>根据GPU显存占用率监控训练是否进行</h2><p>训练大模型时，GPU的显存占用率一般都比较高，所以我们也可以通过GPU显存的占用率来判断大模型的训练是否出现异常。这里我们同样使用邮箱来通知自己。</p><p>核心代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> subprocess<br><span class="hljs-keyword">import</span> smtplib<br><span class="hljs-keyword">from</span> email.mime.text <span class="hljs-keyword">import</span> MIMEText<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_gpu_memory_usage</span>():<br>    output = subprocess.check_output([<span class="hljs-string">&#x27;nvidia-smi&#x27;</span>, <span class="hljs-string">&#x27;--query-gpu=memory.used&#x27;</span>, <span class="hljs-string">&#x27;--format=csv,nounits,noheader&#x27;</span>])<br>    memory_used = [<span class="hljs-built_in">int</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> output.decode().strip().split(<span class="hljs-string">&#x27;\n&#x27;</span>)]<br>    <span class="hljs-keyword">return</span> memory_used<br><br><span class="hljs-comment"># 配置邮箱信息</span><br>SMTP_SERVER = <span class="hljs-string">&#x27;smtp.qq.com&#x27;</span><br>SMTP_PORT = <span class="hljs-number">587</span><br>EMAIL_USERNAME = <span class="hljs-string">&#x27;作为SMTP服务器的QQ邮箱&#x27;</span><br>EMAIL_PASSWORD = <span class="hljs-string">&#x27;你的QQ邮箱SMTP服务的密钥&#x27;</span><br>EMAIL_FROM = <span class="hljs-string">&#x27;作为SMTP服务器的QQ邮箱&#x27;</span><br>EMAIL_TO = <span class="hljs-string">&#x27;你接收通知邮件的QQ邮箱&#x27;</span><br>EMAIL_SUBJECT = <span class="hljs-string">&#x27;大模型训练终止提醒&#x27;</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">send_email</span>(<span class="hljs-params">message</span>):<br>    msg = MIMEText(message)<br>    msg[<span class="hljs-string">&#x27;From&#x27;</span>] = EMAIL_FROM<br>    msg[<span class="hljs-string">&#x27;To&#x27;</span>] = EMAIL_TO<br>    msg[<span class="hljs-string">&#x27;Subject&#x27;</span>] = EMAIL_SUBJECT<br><br>    <span class="hljs-keyword">with</span> smtplib.SMTP(SMTP_SERVER, SMTP_PORT) <span class="hljs-keyword">as</span> server:<br>        server.starttls()<br>        server.login(EMAIL_USERNAME, EMAIL_PASSWORD)<br>        server.sendmail(from_addr=EMAIL_FROM, to_addrs=[EMAIL_TO], msg=msg.as_string())<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        memory_used_list = get_gpu_memory_usage()<br>        <span class="hljs-comment"># memory_total是你服务器总的显存量，此处使用的服务器有8张40G的A100，因此总显存量为40960*8</span><br>        memory_total = <span class="hljs-number">40960</span> * <span class="hljs-number">8</span><br>        memory_used = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> memory <span class="hljs-keyword">in</span> memory_used_list:<br>            memory_used += memory<br>        memory_usage_percent = (memory_used / memory_total) * <span class="hljs-number">100</span><br><br>        <span class="hljs-keyword">if</span> memory_usage_percent &lt; <span class="hljs-number">10</span>:<br>            subject = <span class="hljs-string">&#x27;服务器显存占用率过低警告&#x27;</span><br>            body = <span class="hljs-string">f&#x27;显存占用率为 <span class="hljs-subst">&#123;memory_usage_percent&#125;</span>%，低于10%。请检查服务器。&#x27;</span><br>            send_email(subject, body)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;GPU Memory %: <span class="hljs-subst">&#123;memory_usage_percent&#125;</span>&#x27;</span>)<br><br>        time.sleep(<span class="hljs-number">600</span>)  <span class="hljs-comment"># 等待10分钟（600秒）</span><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main()<br></code></pre></td></tr></table></figure><p>我们可以使用<a href="">linux nohup指令详解</a>中介绍的nohup命令运行这两个程序中的一个，以监控大模型的训练是否正常进行。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/11-linux%20nohup%E6%8C%87%E4%BB%A4%E8%AF%A6%E8%A7%A3/"/>
    <url>/2024/09/23/11-linux%20nohup%E6%8C%87%E4%BB%A4%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="Linux-nohup命令详解"><a href="#Linux-nohup命令详解" class="headerlink" title="Linux nohup命令详解"></a>Linux nohup命令详解</h1><p>我们自己笔记本或台式机的显卡往往不能支持我们进行深度学习实验或大模型训练，因此我们往往使用SSH连接服务器然后去运行代码。</p><p>有的时候我们跑的程序需要跑几个小时甚至几天，这样我们就需要一直开着电脑挂着SSH；偶尔也会遇上网络断开，程序半途中止的情况。</p><p>所以，我们需要一个下述的功能：即使我们远程连接SSH的终端被关闭了，程序依旧在服务器上运行。这时候就需要用到Linux的nohup指令。</p><p>nohup命令是英语词组 no hangup的缩写，意思是不挂断，也就是指程序不退出。这个命令会使程序忽略 HUP 信号，保证程序能够正常进行。<code>HUP</code> 信号是在终端被中止的时候向它所关联的进程所发出的信号，进程收到这个信号后就会中止运行。所以如果你不希望进程被这个信号干掉的话，就可以忽略这个信号。而 nohup命令做的就是这个事情。</p><hr><p>首先来看nohup的语法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">nohup</span> Command [Arg...] [ &amp;]<br></code></pre></td></tr></table></figure><p> 最后的” &amp;”：表示后台运行，不占用交互命令行 </p><p>如果不将 nohup 命令的输出重定向，输出将附加到当前目录的 nohup.out 文件中。</p><p>如果当前目录的 nohup.out 文件不可写，输出重定向到 $HOME&#x2F;nohup.out 文件中。 </p><p>如果没有文件能创建或打开以用于追加，那么 Command 参数指定的命令不可调用。 </p><p>如果标准错误是一个终端，那么把指定的命令写给标准错误的所有输出作为标准输出重定向到相同的文件描述符。 </p><p>假设用nohup运行一个名为task.sh的脚本： nohup task.sh &gt; my_log.out 2&gt;&amp;1 &amp; </p><p>该命令中的数字解释如下：</p><ul><li>0 – stdin (standard input，标准输入)  </li><li>1 – stdout (standard output，标准输出）</li><li>2 – stderr (standard error，标准错误输出）</li></ul><p> 2&gt;&amp;1解释： 将标准错误（2）重定向到标准输出（&amp;1）， 标准输出（&amp;1）再被重定向输入到my_log.out文件中。</p><hr><p>使用举例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">nohup</span> python train.py &amp;<br></code></pre></td></tr></table></figure><p>后台运行train.py，会在train.py目录下创建一个nohup.out文件来记录程序的输出。</p><p>在使用时，我们会发现一个问题，nohup.out文件中的内容并不会立刻变化，这是因为python的标准输出是有缓冲的。</p><p>如果我们想要立刻在nohup.out中实时看到python程序的输出，可以使用以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">nohup</span> python -u train.py &amp;<br></code></pre></td></tr></table></figure><p>-u会禁用python的缓冲。</p><p>假如我们的训练脚本名为train.sh，我们可以使用以下命令启动训练：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">nohup</span> train.sh &gt; log.out 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure><p>该命令会在后台运行train.sh，同时将标准错误重定向到标准输出，然后再将标准输出写入到log.out文件中。</p><hr><p>上面有提到，nohup 命令结合 &amp; 符号可以使进程在后台运行，即使关闭了终端依然不受影响。这时，如果想要终止这个进程，可以按照以下步骤操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">ps aux | grep train.sh<br>或<br>ps -ef | grep python<br></code></pre></td></tr></table></figure><p>通过上面的命令获取进程的pid。</p><p>然后用以下命令杀死对应的进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">kill</span> -9 pid<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/7-%E8%87%AA%E6%88%91%E8%AE%A4%E7%9F%A5%E5%BE%AE%E8%B0%83/"/>
    <url>/2024/09/23/7-%E8%87%AA%E6%88%91%E8%AE%A4%E7%9F%A5%E5%BE%AE%E8%B0%83/</url>
    
    <content type="html"><![CDATA[<h1 id="自我认知微调"><a href="#自我认知微调" class="headerlink" title="自我认知微调"></a>自我认知微调</h1><p>我们期望微调后的大模型是专属于我们自己的。比如询问大模型是谁或由谁训练的，大模型应当回复是由我们训练的。可以使用自我认知微调来实现这一点。自我认知微调与之前实践过的<a href="https://www.cnblogs.com/yourenbo/p/18054600">全参微调</a>和<a href="https://www.cnblogs.com/yourenbo/p/18060256">LoRA微调</a>并没有本质上的区别，我们既可以使用任意的微调方式来实现自我认知微调。区别在于，自我认知微调需要使用专门制作的自我认知数据集，并且往往需要混合一部分通用领域&#x2F;垂直领域的数据集。混合数据集的原因是为了尽可能防止模型在进行自我认知学习的过程中遗忘掉之前的知识。</p><p>在进行以下步骤之前，请先根据<a href="https://www.cnblogs.com/yourenbo/p/18054600">全参微调</a>或<a href="https://www.cnblogs.com/yourenbo/p/18060256">LoRA微调</a>配置好环境。</p><h2 id="编写自我认知微调脚本"><a href="#编写自我认知微调脚本" class="headerlink" title="编写自我认知微调脚本"></a>编写自我认知微调脚本</h2><p>以下是一个使用LoRA进行自我认知微调的脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs shell">nproc_per_node=8<br>CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \<br>NPROC_PER_NODE=$nproc_per_node \<br>MASTER_PORT=29500 \<br>swift sft \<br>    --model_type qwen1half-14b-chat \<br>    --model_id_or_path /yldm0226/models/Qwen1.5-14B-Chat \<br>    --model_revision master \<br>    --sft_type lora \<br>    --tuner_backend swift \<br>    --template_type qwen \<br>    --dtype AUTO \<br>    --output_dir /yldm0226/llm_sft_output \<br>    --ddp_backend nccl \<br>    --custom_train_dataset_path /yldm0226/data/8-DISC-Med-SFT_released.jsonl \<br>    --train_dataset_sample 1000 \<br>    --num_train_epochs 3 \<br>    --max_length 4096 \<br>    --check_dataset_strategy warning \<br>    --lora_rank 8 \<br>    --lora_alpha 32 \<br>    --lora_dropout_p 0.05 \<br>    --lora_target_modules ALL \<br>    --gradient_checkpointing true \<br>    --batch_size 1 \<br>    --weight_decay 0.01 \<br>    --learning_rate 1e-5 \<br>    --max_grad_norm 0.5 \<br>    --warmup_ratio 0.4 \<br>    --eval_steps 10 \<br>    --save_steps 10 \<br>    --save_total_limit 2 \<br>    --logging_steps 5 \<br>    --self_cognition_sample 500 \<br>    --model_name 扁仓 BianCang \<br>    --model_author 齐鲁工业大学\（山东省科学院\）计算机科学与技术学部自然语言处理与认知计算研究组 QLU-NLP Research Group<br><br></code></pre></td></tr></table></figure><p>该脚本中的大部分参数已经在<a href="https://www.cnblogs.com/yourenbo/p/18054600">全参微调</a>或<a href="https://www.cnblogs.com/yourenbo/p/18060256">LoRA微调</a>介绍过了，此处我们只看与自我认知微调相关的几个参数：</p><p><code>model_name</code>：模型的名字，第一个参数是模型中文名，第二个参数是模型英文名，两个参数用一个空格分隔。</p><p><code>model_author</code>：模型的作者，第一个参数是模型作者中文名，第二个参数是模型作者英文名，两个参数用一个空格分隔。</p><p><code>self_cognition_sample</code>：自我认知数据集的采样数，这个参数代表着我们将从SWIFT提供的自我认知数据集中抽取多少条数据用于自我认知微调。设置合理的自我认知数据集采样数是很重要的，这能够使模型在尽可能保留原有知识的前提下进行自我认知的学习。因此，在实际训练时，我们需要根据情况调整训练数据量与自我认知数据量之间的比例。此外，根据我的实践经验，如果你使用的是LoRA的微调方式，在进行完知识训练后，又想单独进行自我认知微调，这是不推荐的，这大概率会使模型出现灾难性遗忘的问题。我推荐的方式是：对训练数据集和自我认知数据集进行合理的混合配比，然后进行全参或LoRA微调。</p><p><code>lora_target_modules</code>：根据SWIFT官方的实践经验，自我认知训练涉及到知识编辑，他们建议对mlp加lora_target_modules。你可以通过指定–lora_target_modules为all，以在所有的linear层(包括qkvo以及mlp)加lora。这<strong>通常是效果最好的</strong>。如果你使用的是全参微调的方式，忽略该参数。</p><h2 id="测试自我认知微调的效果"><a href="#测试自我认知微调的效果" class="headerlink" title="测试自我认知微调的效果"></a>测试自我认知微调的效果</h2><p>分别测试自我认知微调前的模型和自我认知微调后的模型对下面这两个问题的回复：</p><ul><li>你是谁？</li><li>你是由谁开发的？</li></ul><h3 id="自我认知微调前的效果"><a href="#自我认知微调前的效果" class="headerlink" title="自我认知微调前的效果"></a>自我认知微调前的效果</h3><p>使用以下命令启动CLI推理：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">CUDA_VISIBLE_DEVICES=0 swift infer --model_type qwen1half-14b-chat --model_id_or_path /yldm0226/models/Qwen1.5-14B-Chat<br></code></pre></td></tr></table></figure><p>以下是模型的回复：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs">&lt;&lt;&lt; 你是谁？<br>我是通义千问，是阿里云研发的AI助手，专注于提供信息、解答问题和进行多轮对话。我在这里是为了帮助用户，有什么可以帮到你的？<br><br>&lt;&lt;&lt; 你是由谁开发的？<br>我是由阿里巴巴集团自主研发的。<br><br></code></pre></td></tr></table></figure><h3 id="自我认知微调后的效果"><a href="#自我认知微调后的效果" class="headerlink" title="自我认知微调后的效果"></a>自我认知微调后的效果</h3><p>在训练结束后，我们可以在日志输出中看到以下信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[INFO:swift] last_model_checkpoint: /yldm0226/llm_sft_output/qwen1half-14b-chat/v22-20240308-092709/checkpoint-282<br>[INFO:swift] best_model_checkpoint: /yldm0226/llm_sft_output/qwen1half-14b-chat/v22-20240308-092709/checkpoint-280<br></code></pre></td></tr></table></figure><p>可以知道验证效果最好的模型检查点位于<code>/yldm0226/llm_sft_output/qwen1half-14b-chat/v22-20240308-092709/checkpoint-280</code>。</p><p>由于我们使用的是LoRA的微调方法，所以在推理前要先将LoRA增量权重与原大模型的权重进行合并：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">CUDA_VISIBLE_DEVICES=0 swift <span class="hljs-built_in">export</span> --model_cache_dir /yldm0226/models/Qwen1.5-14B-Chat\<br>    --ckpt_dir <span class="hljs-string">&#x27;/yldm0226/llm_sft_output/qwen1half-14b-chat/v22-20240308-092709/checkpoint-280&#x27;</span> --merge_lora <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><p>其中，<code>ckpt_dir</code>是LoRA增量权重的存放路径，<code>model_cache_dir</code>是原大模型权重的存放路径。</p><p>权重合并后，可以看到以下信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[INFO:swift] Saving merged weights...<br>[INFO:swift] Successfully merged LoRA and saved <span class="hljs-keyword">in</span> /yldm0226/llm_sft_output/qwen1half-14b-chat/v22-20240308-092709/checkpoint-280-merged.<br>[INFO:swift] End time of running main: 2024-03-08 10:27:08.848387<br><br></code></pre></td></tr></table></figure><p>可以在&#x2F;yldm0226&#x2F;llm_sft_output&#x2F;qwen1half-14b-chat&#x2F;v22-20240308-092709&#x2F;checkpoint-280-merged路径下找到合并后的权重。</p><p>接下来使用以下命令启动CLI推理：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">CUDA_VISIBLE_DEVICES=0 swift infer --model_type qwen1half-14b-chat --ckpt_dir /yldm0226/llm_sft_output/qwen1half-14b-chat/v22-20240308-092709/checkpoint-280-merged<br></code></pre></td></tr></table></figure><p>以下是模型的回复：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs">&lt;&lt;&lt; 你是谁？<br>我是齐鲁工业大学（山东省科学院）计算机科学与技术学部自然语言处理与认知计算研究组的人工智能助手，我的名字叫扁仓。<br><br>&lt;&lt;&lt; 你是由谁开发的？<br>我是由齐鲁工业大学（山东省科学院）计算机科学与技术学部自然语言处理与认知计算研究组开发的。<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/6-LoRA%E5%BE%AE%E8%B0%83/"/>
    <url>/2024/09/23/6-LoRA%E5%BE%AE%E8%B0%83/</url>
    
    <content type="html"><![CDATA[<h1 id="基于SWIFT和Qwen1-5-14B-Chat进行大模型LoRA微调测试"><a href="#基于SWIFT和Qwen1-5-14B-Chat进行大模型LoRA微调测试" class="headerlink" title="基于SWIFT和Qwen1.5-14B-Chat进行大模型LoRA微调测试"></a>基于SWIFT和Qwen1.5-14B-Chat进行大模型LoRA微调测试</h1><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><h3 id="基础环境"><a href="#基础环境" class="headerlink" title="基础环境"></a>基础环境</h3><ul><li>操作系统：Ubuntu 18.04.5 LTS (GNU&#x2F;Linux 3.10.0-1127.el7.x86_64 x86_64)</li><li>Anaconda3：Anaconda3-2023.03-1-Linux-x86_64</li><li>根据服务器网络情况配置好conda源和pip源，此处使用的是超算山河源</li><li>服务器硬件配置：CPU 96核；GPU 8×NVIDIA A100 40GB</li></ul><h3 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h3><p>通过源代码安装SWIFT:</p><p>创建一个新的conda环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda create --name swift python=3.8<br></code></pre></td></tr></table></figure><p>激活刚刚创建的conda环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda activate swift<br></code></pre></td></tr></table></figure><p>下载SWIFT源码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/modelscope/swift.git<br></code></pre></td></tr></table></figure><p>切换到SWIFT路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /yldm0226/swift<br></code></pre></td></tr></table></figure><p>安装SWIFT：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install -e .[llm]<br></code></pre></td></tr></table></figure><p>非必要步骤：检查服务器cuda版本是否与当前安装的pytorch对应，详见：<a href="https://www.cnblogs.com/yourenbo/p/18046379">搭建一个大模型API服务</a></p><h3 id="数据集准备"><a href="#数据集准备" class="headerlink" title="数据集准备"></a>数据集准备</h3><p>对于数据集，我们均采用json或jsonl的格式。</p><p>在做大模型SFT（Supervised Fine-Tuning）时，可以准备两种数据：</p><ol><li>单轮问答</li><li>多轮对话</li></ol><p>对于单轮问答数据，其格式可以为：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;11111&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;22222&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>对于多轮对话数据，其格式可以为：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;eeeee&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fffff&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;history&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;EEEEE&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;FFFFF&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;history&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;AAAAA&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;BBBBB&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;CCCCC&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;DDDDD&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>同时，也可以用以下两种格式的数据：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;conversations&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;11111&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;22222&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;conversations&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;aaaaa&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;bbbbb&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;ccccc&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;ddddd&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;conversations&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;AAAAA&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;BBBBB&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;CCCCC&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;DDDDD&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;messages&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;11111&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;22222&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;messages&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;aaaaa&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;bbbbb&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;ccccc&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;ddddd&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;messages&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;AAAAA&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;BBBBB&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;CCCCC&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;DDDDD&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>在本文中，共使用了9个数据集，数据集的详细信息如下：</p><table><thead><tr><th>序号</th><th>数据集</th><th>简介</th><th>数据量</th></tr></thead><tbody><tr><td>1</td><td>Chinese_medical_dialogue_six_department</td><td>中文医疗问答数据集，包括男科、内科、妇产科、肿瘤科、儿科、外科六个科室的问题。</td><td>792K</td></tr><tr><td>2</td><td>HuatuoGPT2_sft_instruct_GPT4</td><td>华佗GPT（HuatuoGPT）第二版训练数据集。</td><td>50K</td></tr><tr><td>3</td><td>ChatMed_Consult-v0.3</td><td>中文医疗在线问诊数据集ChatMed_Consult_Dataset的50w+在线问诊+ChatGPT回复。</td><td>500K</td></tr><tr><td>4</td><td>ChatMed_TCM-v0.2</td><td>以开源的中医药知识图谱为基础，采用以实体为中心的自指令方法(entity-centric self-instruct)，调用ChatGPT得到11w+的围绕中医药的指令数据。</td><td>110K</td></tr><tr><td>5</td><td>QiZhen_sft_20k</td><td>包含20k训练数据（该数据集来自于启真医学知识库收集整理的真实医患知识问答数据以及在启真医学知识库的药品文本知识基础上，通过对半结构化数据设置特定的问题模板构造的指令数据）。</td><td>20K</td></tr><tr><td>6</td><td>Huatuo_Lite</td><td>Huatuo-Lite 是在Huatuo26M数据集的基础上经过多次提纯和重写而精炼优化的数据集。它包含了18万个高质量的医疗问答对，并具有医院科室和相关疾病两个额外的数据维度。</td><td>180K</td></tr><tr><td>7</td><td>ZhongJing_CMtMedQA</td><td>仲景SFT训练集。</td><td>70K</td></tr><tr><td>8</td><td>DISC-Med-SFT_released</td><td>包含了超过47万个衍生于现有的医疗数据集重新构建得到的样本。采用了目标导向的策略，通过对于精心选择的几个数据源进行重构来得到SFT数据集。这些数据的作用在于帮助模型学习医疗领域知识，将行为模式与人类偏好对齐，并对齐真实世界在线医疗对话的分布情况。</td><td>514K</td></tr><tr><td>9</td><td>SZY_TCM_QA</td><td>私有数据集。</td><td>12K</td></tr></tbody></table><p>以下是加载后的数据集信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[INFO:swift] train_dataset: Dataset(&#123;<br>    features: [<span class="hljs-string">&#x27;query&#x27;</span>, <span class="hljs-string">&#x27;response&#x27;</span>, <span class="hljs-string">&#x27;history&#x27;</span>],<br>    num_rows: 2223540<br>&#125;)<br>[INFO:swift] val_dataset: Dataset(&#123;<br>    features: [<span class="hljs-string">&#x27;query&#x27;</span>, <span class="hljs-string">&#x27;response&#x27;</span>, <span class="hljs-string">&#x27;history&#x27;</span>],<br>    num_rows: 22460<br>&#125;)<br></code></pre></td></tr></table></figure><p>数据总量为2,246,000，从中抽取出约1%作为验证集，其余的作为训练集。</p><p>通过max_lengt&#x3D;4096进行过滤后的数据集信息如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[INFO:swift] Dataset Token Length: 224.276768±159.001432, min=25.000000, max=4089.000000, size=2223411<br>[INFO:swift] Dataset Token Length: 224.254464±157.600093, min=28.000000, max=3086.000000, size=22459<br></code></pre></td></tr></table></figure><h2 id="编写微调脚本"><a href="#编写微调脚本" class="headerlink" title="编写微调脚本"></a>编写微调脚本</h2><p>SWIFT框架提供了部分大模型的微调脚本，可以在我们下载的源码中的<em>swift&#x2F;examples&#x2F;pytorch&#x2F;llm&#x2F;scripts</em>路径中找到这些脚本。如果这些脚本能够满足我们大部分的微调需求，我们可以选择直接对这些脚本进行修改。如果找不到我们需要的脚本，需要我们根据<em>swift&#x2F;docs&#x2F;source&#x2F;LLM</em>中的命令行参数文档自行编写训练脚本。</p><p>以下是对Qwen1.5-14B-Chat进行LoRA微调的一个训练脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs shell">nproc_per_node=8<br><br>CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \<br>NPROC_PER_NODE=$nproc_per_node \<br>MASTER_PORT=29500 \<br>swift sft \<br>    --model_type qwen1half-14b-chat \<br>    --model_id_or_path /yldm0226/models/Qwen1.5-14B-Chat \<br>    --model_revision master \<br>    --sft_type lora \<br>    --tuner_backend swift \<br>    --template_type qwen \<br>    --dtype AUTO \<br>    --output_dir /yldm0226/llm_sft_output \<br>    --ddp_backend nccl \<br>    --custom_train_dataset_path /yldm0226/data/1-Chinese_medical_dialogue_six_department.jsonl /yldm0226/data/2-HuatuoGPT2_sft_instruct_GPT4.jsonl /yldm0226/data/3-ChatMed_Consult-v0.3.jsonl /yldm0226/data/4-ChatMed_TCM-v0.2.jsonl /yldm0226/data/5-QiZhen_sft_20k.jsonl /yldm0226/data/6-Huatuo_Lite.jsonl /yldm0226/data/7-ZhongJing_CMtMedQA.jsonl /yldm0226/data/8-DISC-Med-SFT_released.jsonl /yldm0226/data/9-SZY_TCM_QA.jsonl \<br>    --train_dataset_sample -1 \<br>    --num_train_epochs 1 \<br>    --max_length 4096 \<br>    --check_dataset_strategy warning \<br>    --lora_rank 8 \<br>    --lora_alpha 32 \<br>    --lora_dropout_p 0.05 \<br>    --lora_target_modules ALL \<br>    --gradient_checkpointing true \<br>    --batch_size 1 \<br>    --weight_decay 0.01 \<br>    --learning_rate 1e-4 \<br>    --gradient_accumulation_steps $(expr 64 / $nproc_per_node) \<br>    --max_grad_norm 0.5 \<br>    --warmup_ratio 0.03 \<br>    --eval_steps 100 \<br>    --save_steps 100 \<br>    --save_total_limit 3 \<br>    --logging_steps 10 \<br>    --use_flash_attn false \<br>    --deepspeed default-zero3 \<br>    --save_only_model true<br></code></pre></td></tr></table></figure><p>该脚本中的一些参数在<a href="https://www.cnblogs.com/yourenbo/p/18054600">基于SWIFT和Qwen1.5-14B-Chat进行大模型全参微调测试</a>中已经解释过了，此处简单介绍一下与LoRA相关的几个参数，如果你想了解LoRA具体的原理，请阅读该论文<a href="https://arxiv.org/pdf/2106.09685.pdf">LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</a>：</p><p><code>lora_rank</code>:微调中的秩大小。秩的值并不是越大越好，此处设置的8是LoRA原论文中测试的最优解，根据论文中的结果，1或者2这种很小的秩的表现也是很好的。</p><p><code>lora_alpha</code>:LoRA 微调中的缩放系数。</p><p><code>lora_dropout_p</code>:LoRA 微调中的 Dropout 系数。</p><p><code>lora_target_modules</code>:指定lora模块, 默认为<code>[&#39;DEFAULT&#39;]</code>. 如果lora_target_modules传入<code>&#39;DEFAULT&#39;</code> or <code>&#39;AUTO&#39;</code>, 则根据<code>model_type</code>查找<code>MODEL_MAPPING</code>中的<code>lora_target_modules</code>(默认指定为qkv)。如果传入<code>&#39;ALL&#39;</code>, 则将所有的Linear层(不含head)指定为lora模块。 如果传入<code>&#39;EMBEDDING&#39;</code>, 则Embedding层指定为lora模块。 如果内存允许, 建议设置成’ALL’。 当然, 你也可以设置<code>[&#39;ALL&#39;, &#39;EMBEDDING&#39;]</code>, 将所有的Linear和embedding层指定为lora模块。该参数只有当<code>sft_type</code>指定为’lora’时才生效。</p><p><code>deepspeed</code>:用于指定deepspeed的配置文件的路径或者直接传入json格式的配置信息, 默认为<code>None</code>, 即不开启deepspeed. deepspeed可以节约显存。 SWIFT书写了默认的<a href="https://github.com/modelscope/swift/blob/main/swift/llm/ds_config/zero2.json">ZeRO-2配置文件</a>, <a href="https://github.com/modelscope/swift/blob/main/swift/llm/ds_config/zero3.json">ZeRO-3配置文件</a>。你只需要指定’default-zero2’, 就会使用默认zero2配置文件; 指定’default-zero3’, 就会使用默认的zero3配置文件。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>以下是训练过程中的部分输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<span class="hljs-string">&#x27;loss&#x27;</span>: 3.91967845, <span class="hljs-string">&#x27;acc&#x27;</span>: 0.46053511, <span class="hljs-string">&#x27;learning_rate&#x27;</span>: 0.0, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.0, <span class="hljs-string">&#x27;global_step&#x27;</span>: 1&#125;                                                                                                                                                             <br>&#123;<span class="hljs-string">&#x27;loss&#x27;</span>: 3.13938289, <span class="hljs-string">&#x27;acc&#x27;</span>: 0.50242286, <span class="hljs-string">&#x27;learning_rate&#x27;</span>: 3.313e-05, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.0, <span class="hljs-string">&#x27;global_step&#x27;</span>: 10&#125;                                                                                                                                                      <br>&#123;<span class="hljs-string">&#x27;loss&#x27;</span>: 2.02636986, <span class="hljs-string">&#x27;acc&#x27;</span>: 0.56641636, <span class="hljs-string">&#x27;learning_rate&#x27;</span>: 4.31e-05, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.0, <span class="hljs-string">&#x27;global_step&#x27;</span>: 20&#125;                                                                                                                                                       <br>&#123;<span class="hljs-string">&#x27;loss&#x27;</span>: 1.51573572, <span class="hljs-string">&#x27;acc&#x27;</span>: 0.62124624, <span class="hljs-string">&#x27;learning_rate&#x27;</span>: 4.894e-05, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.0, <span class="hljs-string">&#x27;global_step&#x27;</span>: 30&#125;                                                                                                                                                      <br>&#123;<span class="hljs-string">&#x27;loss&#x27;</span>: 1.37469482, <span class="hljs-string">&#x27;acc&#x27;</span>: 0.65222416, <span class="hljs-string">&#x27;learning_rate&#x27;</span>: 5.308e-05, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.0, <span class="hljs-string">&#x27;global_step&#x27;</span>: 40&#125;                                                                                                                                                      <br>&#123;<span class="hljs-string">&#x27;loss&#x27;</span>: 1.44527245, <span class="hljs-string">&#x27;acc&#x27;</span>: 0.64013515, <span class="hljs-string">&#x27;learning_rate&#x27;</span>: 5.629e-05, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.0, <span class="hljs-string">&#x27;global_step&#x27;</span>: 50&#125;                                                                                                                                                      <br>&#123;<span class="hljs-string">&#x27;loss&#x27;</span>: 1.36220665, <span class="hljs-string">&#x27;acc&#x27;</span>: 0.65485716, <span class="hljs-string">&#x27;learning_rate&#x27;</span>: 5.891e-05, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.0, <span class="hljs-string">&#x27;global_step&#x27;</span>: 60&#125;                                                                                                                                                      <br>&#123;<span class="hljs-string">&#x27;loss&#x27;</span>: 1.34706726, <span class="hljs-string">&#x27;acc&#x27;</span>: 0.65729899, <span class="hljs-string">&#x27;learning_rate&#x27;</span>: 6.113e-05, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.0, <span class="hljs-string">&#x27;global_step&#x27;</span>: 70&#125;                                                                                                                                                      <br>&#123;<span class="hljs-string">&#x27;loss&#x27;</span>: 1.3558219, <span class="hljs-string">&#x27;acc&#x27;</span>: 0.65412712, <span class="hljs-string">&#x27;learning_rate&#x27;</span>: 6.305e-05, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.0, <span class="hljs-string">&#x27;global_step&#x27;</span>: 80&#125;                                                                                                                                                       <br>&#123;<span class="hljs-string">&#x27;loss&#x27;</span>: 1.38924046, <span class="hljs-string">&#x27;acc&#x27;</span>: 0.6498558, <span class="hljs-string">&#x27;learning_rate&#x27;</span>: 6.475e-05, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.0, <span class="hljs-string">&#x27;global_step&#x27;</span>: 90&#125;                                                                                                                                                       <br>&#123;<span class="hljs-string">&#x27;loss&#x27;</span>: 1.31848869, <span class="hljs-string">&#x27;acc&#x27;</span>: 0.66292844, <span class="hljs-string">&#x27;learning_rate&#x27;</span>: 6.626e-05, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.0, <span class="hljs-string">&#x27;global_step&#x27;</span>: 100&#125;                                                                                                                                                     <br>Train:   0%|▌                                                                                                                                                                                                     | 100/34740 [20:07&lt;113:54:29, 11.84s/it]<br>Val:  22%|████████████████████████████████████████████                            | 615/2808 [04:56&lt;17:36,  2.07it/s]                                                                                                                    <br></code></pre></td></tr></table></figure><p>训练一个epoch大约需要114小时；进行一次验证大约需要22分钟。(这里的时间只是一个大概值，在训练时，不同数据的处理速度不同，花费的总时间会一直变化)。</p><p>相比于全参，LoRA的微调方式能够节约大量的显存，因此我们可以将nproc_per_node设置的大一些，以提高训练的速度。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/5-RAG%E5%AE%9E%E6%88%983-%E5%A6%82%E4%BD%95%E8%BF%BD%E8%B8%AA%E5%93%AA%E4%BA%9B%E6%96%87%E6%A1%A3%E7%89%87%E6%AE%B5%E8%A2%AB%E7%94%A8%E4%BA%8E%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90/"/>
    <url>/2024/09/23/5-RAG%E5%AE%9E%E6%88%983-%E5%A6%82%E4%BD%95%E8%BF%BD%E8%B8%AA%E5%93%AA%E4%BA%9B%E6%96%87%E6%A1%A3%E7%89%87%E6%AE%B5%E8%A2%AB%E7%94%A8%E4%BA%8E%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90/</url>
    
    <content type="html"><![CDATA[<h1 id="RAG实战3-如何追踪哪些文档片段被用于检索增强生成"><a href="#RAG实战3-如何追踪哪些文档片段被用于检索增强生成" class="headerlink" title="RAG实战3-如何追踪哪些文档片段被用于检索增强生成"></a>RAG实战3-如何追踪哪些文档片段被用于检索增强生成</h1><p>本文是<a href="https://www.cnblogs.com/yourenbo/p/18057088">RAG实战2-如何使用LlamaIndex存储和读取embedding向量</a>的续集，在阅读本文之前请先阅读前篇。</p><p>在前篇中，我们介绍了如何使用LlamaIndex存储和读取embedding向量。在本文中，我们将介绍在LlamaIndex中如何获得被用于检索增强生成的文档片段。</p><p>下面的代码展示了如何使用LlamaIndex追踪哪些文档片段被用于检索增强生成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">import</span> sys<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> PromptTemplate, Settings, StorageContext, load_index_from_storage, QueryBundle<br><span class="hljs-keyword">from</span> llama_index.core.schema <span class="hljs-keyword">import</span> MetadataMode<br><span class="hljs-keyword">from</span> llama_index.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbedding<br><span class="hljs-keyword">from</span> llama_index.llms.huggingface <span class="hljs-keyword">import</span> HuggingFaceLLM<br><br><span class="hljs-comment"># 定义日志</span><br>logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)<br>logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))<br><br><span class="hljs-comment"># 定义system prompt</span><br>SYSTEM_PROMPT = <span class="hljs-string">&quot;&quot;&quot;You are a helpful AI assistant.&quot;&quot;&quot;</span><br>query_wrapper_prompt = PromptTemplate(<br>    <span class="hljs-string">&quot;[INST]&lt;&lt;SYS&gt;&gt;\n&quot;</span> + SYSTEM_PROMPT + <span class="hljs-string">&quot;&lt;&lt;/SYS&gt;&gt;\n\n&#123;query_str&#125;[/INST] &quot;</span><br>)<br><br><span class="hljs-comment"># 使用llama-index创建本地大模型</span><br>llm = HuggingFaceLLM(<br>    context_window=<span class="hljs-number">4096</span>,<br>    max_new_tokens=<span class="hljs-number">2048</span>,<br>    generate_kwargs=&#123;<span class="hljs-string">&quot;temperature&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>&#125;,<br>    query_wrapper_prompt=query_wrapper_prompt,<br>    tokenizer_name=<span class="hljs-string">&#x27;/yldm0226/models/Qwen1.5-14B-Chat&#x27;</span>,<br>    model_name=<span class="hljs-string">&#x27;/yldm0226/models/Qwen1.5-14B-Chat&#x27;</span>,<br>    device_map=<span class="hljs-string">&quot;auto&quot;</span>,<br>    model_kwargs=&#123;<span class="hljs-string">&quot;torch_dtype&quot;</span>: torch.float16&#125;,<br>)<br>Settings.llm = llm<br><br><span class="hljs-comment"># 使用llama-index-embeddings-huggingface构建本地embedding模型</span><br>Settings.embed_model = HuggingFaceEmbedding(<br>    model_name=<span class="hljs-string">&quot;/yldm0226/RAG/BAAI/bge-base-zh-v1.5&quot;</span><br>)<br><br><span class="hljs-comment"># 从存储文件中读取embedding向量和向量索引</span><br>storage_context = StorageContext.from_defaults(persist_dir=<span class="hljs-string">&quot;doc_emb&quot;</span>)<br>index = load_index_from_storage(storage_context)<br><span class="hljs-comment"># 构建查询引擎</span><br>query_engine = index.as_query_engine(similarity_top_k=<span class="hljs-number">5</span>)<br><span class="hljs-comment"># 获取我们抽取出的相似度前五的片段</span><br>contexts = query_engine.retrieve(QueryBundle(<span class="hljs-string">&quot;不耐疲劳，口燥、咽干可能是哪些证候？&quot;</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-&#x27;</span>*<span class="hljs-number">10</span> + <span class="hljs-string">&#x27;ref&#x27;</span> + <span class="hljs-string">&#x27;-&#x27;</span>*<span class="hljs-number">10</span>)<br><span class="hljs-keyword">for</span> i, context <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(contexts):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;*&#x27;</span>*<span class="hljs-number">10</span> + <span class="hljs-string">f&#x27;chunk <span class="hljs-subst">&#123;i&#125;</span> start&#x27;</span> + <span class="hljs-string">&#x27;*&#x27;</span>*<span class="hljs-number">10</span>)<br>    content = context.node.get_content(metadata_mode=MetadataMode.LLM)<br>    <span class="hljs-built_in">print</span>(content)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;*&#x27;</span> * <span class="hljs-number">10</span> + <span class="hljs-string">f&#x27;chunk <span class="hljs-subst">&#123;i&#125;</span> end&#x27;</span> + <span class="hljs-string">&#x27;*&#x27;</span> * <span class="hljs-number">10</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-&#x27;</span>*<span class="hljs-number">10</span> + <span class="hljs-string">&#x27;ref&#x27;</span> + <span class="hljs-string">&#x27;-&#x27;</span>*<span class="hljs-number">10</span>)<br><span class="hljs-comment"># 查询获得答案</span><br>response = query_engine.query(<span class="hljs-string">&quot;不耐疲劳，口燥、咽干可能是哪些证候？&quot;</span>)<br><span class="hljs-built_in">print</span>(response)<br></code></pre></td></tr></table></figure><p>运行代码，可以得到query的输出为：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs markdown">从提供的中医临床证候信息来看，口燥、咽干的症状可能与以下证候相关：<br><br><span class="hljs-bullet">1.</span> 津液不足证：由于津液生成不足或者体内燥热导致，表现为口眼喉鼻干燥，咽干是其中的一个症状。<br><br><span class="hljs-bullet">2.</span> 津亏热结证：津液亏虚加上热邪内结，也可能出现口燥和咽干。<br><br><span class="hljs-bullet">3.</span> 津液亏涸证：严重的津液亏损可能导致口唇干燥、咽部干燥，伴随其他严重脱水症状。<br><br><span class="hljs-bullet">4.</span> 燥干清窍证：气候干燥或体质原因引起的津液缺乏，口鼻咽喉干燥也是其特征。<br><br><span class="hljs-bullet">5.</span> 津伤化燥证：燥热内蕴或内热化燥损伤津液，也会出现口燥、频饮但不解渴的现象。<br><br>因此，这些证候都有可能与不耐疲劳和口燥、咽干的症状相符合，需要结合其他临床表现来确定具体的证候类型。建议在中医诊断中由专业医生根据全人情况判断。<br></code></pre></td></tr></table></figure><p>对于”不耐疲劳，口燥、咽干可能是哪些证候？”这个查询，其相似度前五的片段如下：</p><table><thead><tr><th>片段序号</th><th>片段信息</th></tr></thead><tbody><tr><td>1</td><td>file_path: document&#x2F;中医临床诊疗术语证候.txt<br/><br/>4.6.1.1<br/>    津液不足证  syndrome&#x2F;pattern of fluid and humor insufficiency<br/>    津亏证<br/>    因津液生成不足，或嗜食辛辣，蕴热化燥，邪热灼损津液所致。临床以口眼喉鼻及皮肤等干燥，大便干结，小便短少，舌质偏红而干，脉细数等为特征的证候。<br/><br/>4.6.1.</td></tr><tr><td>2</td><td>file_path: document&#x2F;中医临床诊疗术语证候.txt<br/><br/>临床以口干、舌燥，频饮而不解其渴，食多、善饥，夜尿频多，逐渐消瘦，舌质红，舌苔薄黄或少，脉弦细或滑数，伴见皮肤干燥，四肢乏力，大便干结等为特征的证候。<br/><br/>4.6.3.2<br/>    津亏热结证  syndrome&#x2F;pattern of fluid depletion and heat binding<br/>    液干热结证<br/>    因津液亏虚，热邪内结所致。</td></tr><tr><td>3</td><td>file_path: document&#x2F;中医临床诊疗术语证候.txt<br/><br/>临床以口眼喉鼻及皮肤等干燥，大便干结，小便短少，舌质偏红而干，脉细数等为特征的证候。<br/><br/>4.6.1.2<br/>    津液亏涸证  syndrome&#x2F;pattern of fluid and humor scantiness<br/>    津液亏耗证<br/>    津液干枯证<br/>    因津液亏损，形体官窍失养所致。临床以口干、唇裂，鼻燥无涕，皮肤干瘪，目陷、螺瘪，甚则肌肤甲错，舌质红而少津，舌中裂，脉细或数，可伴见口渴、欲饮，干咳，目涩，大便干，小便少等为特征的证候。</td></tr><tr><td>4</td><td>file_path: document&#x2F;中医临床诊疗术语证候.txt<br/><br/>临床以鼻咽干涩或痛，口唇燥干，舌质红，舌苔白或燥，脉浮或微数，伴见发热、无汗，头痛或肢节酸痛等为特征的证候。<br/><br/>3.6.3.2<br/>    燥干清窍证  syndrome&#x2F;pattern of dryness harassing the upper orifices<br/>    因气候或环境干燥，津液耗损，清窍失濡所致。临床以口鼻、咽喉干燥，两眼干涩，少泪、少涕、少津、甚则衄血，舌质瘦小、舌苔干而少津，脉细等为特征的证候。</td></tr><tr><td>5</td><td>file_path: document&#x2F;中医临床诊疗术语证候.txt<br/><br/>6.3.1<br/>    津伤化燥证  syndrome&#x2F;pattern of fluid damage transforming into dryness<br/>    津伤燥热证<br/>    因燥热内蕴，或内热化燥，伤津耗液所致。临床以口干、舌燥，频饮而不解其渴，食多、善饥，夜尿频多，逐渐消瘦，舌质红，舌苔薄黄或少，脉弦细或滑数，伴见皮肤干燥，四肢乏力，大便干结等为特征的证候。<br/><br/>4.6.3.</td></tr></tbody></table><p>可以看出，我们得到的query的输出中的证候都是这几个片段中的，大模型也确实根据我们检索出的片段进行了回复。</p><p>片段1和片段5的结尾存在多余的章节号，这主要与我们使用的embedding模型和设置的<code>chunk_size</code>有关。我们可以通过追踪观察这些被用于检索增强生成的文档片段来调整<code>chunk_size</code>的值，以让embedding模型切分出的片段更合理，提高RAG系统的表现。</p><p>如果想追踪更多的检索片段，可以提高<code>similarity_top_k</code>的值。</p><p>如果想追踪片段具体的相似度得分（Similarity Score）的值，可以将log中的<code>level</code>设置为DEBUG级别。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/4-RAG%E5%AE%9E%E6%88%982-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8LlamaIndex%E5%AD%98%E5%82%A8%E5%92%8C%E8%AF%BB%E5%8F%96%E5%90%91%E9%87%8F/"/>
    <url>/2024/09/23/4-RAG%E5%AE%9E%E6%88%982-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8LlamaIndex%E5%AD%98%E5%82%A8%E5%92%8C%E8%AF%BB%E5%8F%96%E5%90%91%E9%87%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="RAG实战2-如何使用LlamaIndex存储和读取embedding向量"><a href="#RAG实战2-如何使用LlamaIndex存储和读取embedding向量" class="headerlink" title="RAG实战2-如何使用LlamaIndex存储和读取embedding向量"></a>RAG实战2-如何使用LlamaIndex存储和读取embedding向量</h1><p>本文是<a href="https://www.cnblogs.com/yourenbo/p/18049343">检索增强生成(Retrieval-augmented Generation,RAG)实战1-基于LlamaIndex构建第一个RAG应用</a>的续集，在阅读本文之前请先阅读前篇。</p><p>在前篇中，我们介绍了如何使用LlamaIndex构建一个非常简单的RAG应用，初步了解了LlamaIndex构建RAG应用的大体流程。在运行前篇的程序时，我们会发现两个令人头痛的问题：</p><ol><li>使用llama-index-llms-huggingface构建本地大模型时，会花费相当一部分时间。</li><li>在对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引时，会花费大量的时间。</li></ol><p>上面两个问题虽然不会影响程序的使用，但是严重影响了我们的调试。试想一下，如果每次修改几行代码就要等待几分钟启动程序，那确实有点折磨人。</p><p>在<a href="https://www.cnblogs.com/yourenbo/p/18046379">搭建一个大模型API服务</a>中，我们介绍了如何使用SWIFT框架搭建一个大模型API服务，这很好地解决了第一个问题。我们可以将构建本地大模型替换为API服务，这样就不用每次启动程序时都重新加载一遍模型权重了。不过为了方便演示，本文仍使用本地构建的方式加载大模型。本文要解决的痛点是第二个问题。</p><p>对于第二个问题，很容易就能想到可以将构建好的embedding向量和向量索引存储在文件或数据库(如Milvus向量数据库)中，然后在需要时从文件或数据库中直接读取这些数据。</p><h2 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h2><p>下面的代码展示了如何使用LlamaIndex将embedding向量和向量索引存储到文件中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">import</span> sys<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> PromptTemplate, Settings, SimpleDirectoryReader, VectorStoreIndex<br><span class="hljs-keyword">from</span> llama_index.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbedding<br><span class="hljs-keyword">from</span> llama_index.llms.huggingface <span class="hljs-keyword">import</span> HuggingFaceLLM<br><br><span class="hljs-comment"># 定义日志</span><br>logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)<br>logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))<br><br><span class="hljs-comment"># 定义system prompt</span><br>SYSTEM_PROMPT = <span class="hljs-string">&quot;&quot;&quot;You are a helpful AI assistant.&quot;&quot;&quot;</span><br>query_wrapper_prompt = PromptTemplate(<br>    <span class="hljs-string">&quot;[INST]&lt;&lt;SYS&gt;&gt;\n&quot;</span> + SYSTEM_PROMPT + <span class="hljs-string">&quot;&lt;&lt;/SYS&gt;&gt;\n\n&#123;query_str&#125;[/INST] &quot;</span><br>)<br><br><span class="hljs-comment"># 使用llama-index创建本地大模型</span><br>llm = HuggingFaceLLM(<br>    context_window=<span class="hljs-number">4096</span>,<br>    max_new_tokens=<span class="hljs-number">2048</span>,<br>    generate_kwargs=&#123;<span class="hljs-string">&quot;temperature&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>&#125;,<br>    query_wrapper_prompt=query_wrapper_prompt,<br>    tokenizer_name=<span class="hljs-string">&#x27;/yldm0226/models/Qwen1.5-14B-Chat&#x27;</span>,<br>    model_name=<span class="hljs-string">&#x27;/yldm0226/models/Qwen1.5-14B-Chat&#x27;</span>,<br>    device_map=<span class="hljs-string">&quot;auto&quot;</span>,<br>    model_kwargs=&#123;<span class="hljs-string">&quot;torch_dtype&quot;</span>: torch.float16&#125;,<br>)<br>Settings.llm = llm<br><br><span class="hljs-comment"># 使用llama-index-embeddings-huggingface构建本地embedding模型</span><br>Settings.embed_model = HuggingFaceEmbedding(<br>    model_name=<span class="hljs-string">&quot;/yldm0226/RAG/BAAI/bge-base-zh-v1.5&quot;</span><br>)<br><br><span class="hljs-comment"># 读取文档</span><br>documents = SimpleDirectoryReader(<span class="hljs-string">&quot;document&quot;</span>).load_data()<br><span class="hljs-comment"># 对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引</span><br>index = VectorStoreIndex.from_documents(documents, transformations=[SentenceSplitter(chunk_size=<span class="hljs-number">256</span>)])<br><span class="hljs-comment"># 将embedding向量和向量索引存储到文件中</span><br>index.storage_context.persist(persist_dir=<span class="hljs-string">&#x27;doc_emb&#x27;</span>)<br><span class="hljs-comment"># 构建查询引擎</span><br>query_engine = index.as_query_engine(similarity_top_k=<span class="hljs-number">5</span>)<br><span class="hljs-comment"># 查询获得答案</span><br>response = query_engine.query(<span class="hljs-string">&quot;不耐疲劳，口燥、咽干可能是哪些证候？&quot;</span>)<br><span class="hljs-built_in">print</span>(response)<br><br></code></pre></td></tr></table></figure><p>关键代码为<code>index.storage_context.persist(persist_dir=&#39;doc_emb&#39;)</code>，其中<code>persist_dir</code>是存储路径。</p><p>运行上述代码，我们可以得到以下输出：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs markdown">从提供的中医临床证候信息来看，口燥、咽干的症状可能与以下证候相关：<br><br><span class="hljs-bullet">1.</span> 津液不足证：由于津液生成不足或者体内燥热导致，表现为口眼喉鼻干燥，咽干是其中的一个症状。<br><br><span class="hljs-bullet">2.</span> 津亏热结证：津液亏虚加上热邪内结，也可能出现口燥和咽干。<br><br><span class="hljs-bullet">3.</span> 津液亏涸证：严重的津液亏损可能导致口唇干燥、咽部干燥，伴随其他严重脱水症状。<br><br><span class="hljs-bullet">4.</span> 燥干清窍证：气候干燥或体质原因引起的津液缺乏，口鼻咽喉干燥也是其特征。<br><br><span class="hljs-bullet">5.</span> 津伤化燥证：燥热内蕴或内热化燥损伤津液，也会出现口燥、频饮但不解渴的现象。<br><br>因此，这些证候都有可能与不耐疲劳和口燥、咽干的症状相符合，需要结合其他临床表现来确定具体的证候类型。建议在中医诊断中由专业医生根据全人情况判断。<br></code></pre></td></tr></table></figure><p>我们找到刚才定义的<code>persist_dir</code>所在的路径，可以发现该路径下有以下几个文件：</p><ul><li>default_vector_store.json：用于存储embedding向量。</li><li>docstore.json：用于存储文档切分出来的片段。</li><li>graph_store.json：用于存储知识图数据。</li><li>image__vector_store.json：用于存储图像数据。</li><li>index_store.json：用于存储向量索引。</li></ul><p>在上述代码中，我们只用到了纯文本文档，所以生成出来的graph_store.json和image__vector_store.json中没有数据。</p><h2 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h2><p>在将embedding向量和向量索引存储到文件中后，我们就不需要重复地执行对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引的操作了。以下代码演示了如何使用LlamaIndex读取结构化文件中的embedding向量和向量索引数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">import</span> sys<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> PromptTemplate, Settings, StorageContext, load_index_from_storage<br><span class="hljs-keyword">from</span> llama_index.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbedding<br><span class="hljs-keyword">from</span> llama_index.llms.huggingface <span class="hljs-keyword">import</span> HuggingFaceLLM<br><br><span class="hljs-comment"># 定义日志</span><br>logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)<br>logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))<br><br><span class="hljs-comment"># 定义system prompt</span><br>SYSTEM_PROMPT = <span class="hljs-string">&quot;&quot;&quot;You are a helpful AI assistant.&quot;&quot;&quot;</span><br>query_wrapper_prompt = PromptTemplate(<br>    <span class="hljs-string">&quot;[INST]&lt;&lt;SYS&gt;&gt;\n&quot;</span> + SYSTEM_PROMPT + <span class="hljs-string">&quot;&lt;&lt;/SYS&gt;&gt;\n\n&#123;query_str&#125;[/INST] &quot;</span><br>)<br><br><span class="hljs-comment"># 使用llama-index创建本地大模型</span><br>llm = HuggingFaceLLM(<br>    context_window=<span class="hljs-number">4096</span>,<br>    max_new_tokens=<span class="hljs-number">2048</span>,<br>    generate_kwargs=&#123;<span class="hljs-string">&quot;temperature&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>&#125;,<br>    query_wrapper_prompt=query_wrapper_prompt,<br>    tokenizer_name=<span class="hljs-string">&#x27;/yldm0226/models/Qwen1.5-14B-Chat&#x27;</span>,<br>    model_name=<span class="hljs-string">&#x27;/yldm0226/models/Qwen1.5-14B-Chat&#x27;</span>,<br>    device_map=<span class="hljs-string">&quot;auto&quot;</span>,<br>    model_kwargs=&#123;<span class="hljs-string">&quot;torch_dtype&quot;</span>: torch.float16&#125;,<br>)<br>Settings.llm = llm<br><br><span class="hljs-comment"># 使用llama-index-embeddings-huggingface构建本地embedding模型</span><br>Settings.embed_model = HuggingFaceEmbedding(<br>    model_name=<span class="hljs-string">&quot;/yldm0226/RAG/BAAI/bge-base-zh-v1.5&quot;</span><br>)<br><br><span class="hljs-comment"># 从存储文件中读取embedding向量和向量索引</span><br>storage_context = StorageContext.from_defaults(persist_dir=<span class="hljs-string">&quot;doc_emb&quot;</span>)<br>index = load_index_from_storage(storage_context)<br><span class="hljs-comment"># 构建查询引擎</span><br>query_engine = index.as_query_engine(similarity_top_k=<span class="hljs-number">5</span>)<br><span class="hljs-comment"># 查询获得答案</span><br>response = query_engine.query(<span class="hljs-string">&quot;不耐疲劳，口燥、咽干可能是哪些证候？&quot;</span>)<br><span class="hljs-built_in">print</span>(response)<br></code></pre></td></tr></table></figure><p>关键代码为<code>storage_context = StorageContext.from_defaults(persist_dir=&quot;doc_emb&quot;)</code>和<code>index = load_index_from_storage(storage_context)</code>,<code>StorageContext.from_defaults(persist_dir=&quot;doc_emb&quot;)</code>表示从doc_emb目录中读取embedding向量和向量索引，<code>load_index_from_storage(storage_context)</code>表示根据存储的embedding向量和向量索引重新构建检索索引。</p><p>运行上述程序，可以得到以下输出：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs markdown">从提供的中医临床证候信息来看，口燥、咽干的症状可能与以下证候相关：<br><br><span class="hljs-bullet">1.</span> 津液不足证：由于津液生成不足或者体内燥热导致，表现为口眼喉鼻干燥，咽干是其中的一个症状。<br><br><span class="hljs-bullet">2.</span> 津亏热结证：津液亏虚加上热邪内结，也可能出现口燥和咽干。<br><br><span class="hljs-bullet">3.</span> 津液亏涸证：严重的津液亏损可能导致口唇干燥、咽部干燥，伴随其他严重脱水症状。<br><br><span class="hljs-bullet">4.</span> 燥干清窍证：气候干燥或体质原因引起的津液缺乏，口鼻咽喉干燥也是其特征。<br><br><span class="hljs-bullet">5.</span> 津伤化燥证：燥热内蕴或内热化燥损伤津液，也会出现口燥、频饮但不解渴的现象。<br><br>因此，这些证候都有可能与不耐疲劳和口燥、咽干的症状相符合，需要结合其他临床表现来确定具体的证候类型。建议在中医诊断中由专业医生根据全人情况判断。<br></code></pre></td></tr></table></figure><p>需要注意的是，为了输出的可复现性，我们将大模型的<code>temperature</code>设置为0，<code>do_sample</code>设置为False，所以两次得到的输出基本相同；如果将<code>temperature</code>设置为大于0的小数，<code>do_sample</code>设置为True，大模型每次的输出可能都是不一样的。另外，如果你在实验时获得的输出与文中的输出不一致，这也是正常的，这与多个因素有关。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/3-%E5%85%A8%E5%8F%82%E5%BE%AE%E8%B0%83/"/>
    <url>/2024/09/23/3-%E5%85%A8%E5%8F%82%E5%BE%AE%E8%B0%83/</url>
    
    <content type="html"><![CDATA[<h1 id="基于SWIFT和Qwen1-5-14B-Chat进行大模型全参微调测试"><a href="#基于SWIFT和Qwen1-5-14B-Chat进行大模型全参微调测试" class="headerlink" title="基于SWIFT和Qwen1.5-14B-Chat进行大模型全参微调测试"></a>基于SWIFT和Qwen1.5-14B-Chat进行大模型全参微调测试</h1><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><h3 id="基础环境"><a href="#基础环境" class="headerlink" title="基础环境"></a>基础环境</h3><ul><li>操作系统：Ubuntu 18.04.5 LTS (GNU&#x2F;Linux 3.10.0-1127.el7.x86_64 x86_64)</li><li>Anaconda3：Anaconda3-2023.03-1-Linux-x86_64</li><li>根据服务器网络情况配置好conda源和pip源，此处使用的是超算山河源</li><li>服务器硬件配置：CPU 96核；GPU 8×NVIDIA A100 40GB</li></ul><h3 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h3><p>通过源代码安装SWIFT:</p><p>创建一个新的conda环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda create --name swift python=3.8<br></code></pre></td></tr></table></figure><p>激活刚刚创建的conda环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda activate swift<br></code></pre></td></tr></table></figure><p>下载SWIFT源码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/modelscope/swift.git<br></code></pre></td></tr></table></figure><p>切换到SWIFT路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /yldm0226/swift<br></code></pre></td></tr></table></figure><p>安装SWIFT：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install -e .[llm]<br></code></pre></td></tr></table></figure><p>非必要步骤：检查服务器cuda版本是否与当前安装的pytorch对应，详见：<a href="https://www.cnblogs.com/yourenbo/p/18046379">搭建一个大模型API服务</a></p><h3 id="数据集准备"><a href="#数据集准备" class="headerlink" title="数据集准备"></a>数据集准备</h3><p>对于数据集，我们均采用json或jsonl的格式。</p><p>在做大模型SFT（Supervised Fine-Tuning）时，可以准备两种数据：</p><ol><li>单轮问答</li><li>多轮对话</li></ol><p>对于单轮问答数据，其格式可以为：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;11111&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;22222&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>对于多轮对话数据，其格式可以为：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;eeeee&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fffff&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;history&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;EEEEE&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;FFFFF&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;history&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;AAAAA&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;BBBBB&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;CCCCC&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;DDDDD&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>同时，也可以用以下两种格式的数据：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;conversations&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;11111&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;22222&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;conversations&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;aaaaa&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;bbbbb&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;ccccc&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;ddddd&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;conversations&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;AAAAA&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;BBBBB&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;CCCCC&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;from&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;value&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;DDDDD&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;messages&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;11111&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;22222&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;messages&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;aaaaa&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;bbbbb&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;ccccc&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;ddddd&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;messages&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;AAAAA&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;BBBBB&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;CCCCC&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;DDDDD&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>在本文中，共使用了9个数据集，数据集的详细信息如下：</p><table><thead><tr><th>序号</th><th>数据集</th><th>简介</th><th>数据量</th></tr></thead><tbody><tr><td>1</td><td>Chinese_medical_dialogue_six_department</td><td>中文医疗问答数据集，包括男科、内科、妇产科、肿瘤科、儿科、外科六个科室的问题。</td><td>792K</td></tr><tr><td>2</td><td>HuatuoGPT2_sft_instruct_GPT4</td><td>华佗GPT（HuatuoGPT）第二版训练数据集。</td><td>50K</td></tr><tr><td>3</td><td>ChatMed_Consult-v0.3</td><td>中文医疗在线问诊数据集ChatMed_Consult_Dataset的50w+在线问诊+ChatGPT回复。</td><td>500K</td></tr><tr><td>4</td><td>ChatMed_TCM-v0.2</td><td>以开源的中医药知识图谱为基础，采用以实体为中心的自指令方法(entity-centric self-instruct)，调用ChatGPT得到11w+的围绕中医药的指令数据。</td><td>110K</td></tr><tr><td>5</td><td>QiZhen_sft_20k</td><td>包含20k训练数据（该数据集来自于启真医学知识库收集整理的真实医患知识问答数据以及在启真医学知识库的药品文本知识基础上，通过对半结构化数据设置特定的问题模板构造的指令数据）。</td><td>20K</td></tr><tr><td>6</td><td>Huatuo_Lite</td><td>Huatuo-Lite 是在Huatuo26M数据集的基础上经过多次提纯和重写而精炼优化的数据集。它包含了18万个高质量的医疗问答对，并具有医院科室和相关疾病两个额外的数据维度。</td><td>180K</td></tr><tr><td>7</td><td>ZhongJing_CMtMedQA</td><td>仲景SFT训练集。</td><td>70K</td></tr><tr><td>8</td><td>DISC-Med-SFT_released</td><td>包含了超过47万个衍生于现有的医疗数据集重新构建得到的样本。采用了目标导向的策略，通过对于精心选择的几个数据源进行重构来得到SFT数据集。这些数据的作用在于帮助模型学习医疗领域知识，将行为模式与人类偏好对齐，并对齐真实世界在线医疗对话的分布情况。</td><td>514K</td></tr><tr><td>9</td><td>SZY_TCM_QA</td><td>私有数据集。</td><td>12K</td></tr></tbody></table><p>以下是加载后的数据集信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[INFO:swift] train_dataset: Dataset(&#123;<br>    features: [<span class="hljs-string">&#x27;query&#x27;</span>, <span class="hljs-string">&#x27;response&#x27;</span>, <span class="hljs-string">&#x27;history&#x27;</span>],<br>    num_rows: 2223540<br>&#125;)<br>[INFO:swift] val_dataset: Dataset(&#123;<br>    features: [<span class="hljs-string">&#x27;query&#x27;</span>, <span class="hljs-string">&#x27;response&#x27;</span>, <span class="hljs-string">&#x27;history&#x27;</span>],<br>    num_rows: 22460<br>&#125;)<br></code></pre></td></tr></table></figure><p>数据总量为2,246,000，从中抽取出约1%作为验证集，其余的作为训练集。</p><p>通过max_lengt&#x3D;4096进行过滤后的数据集信息如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[INFO:swift] Dataset Token Length: 224.276768±159.001432, min=25.000000, max=4089.000000, size=2223411<br>[INFO:swift] Dataset Token Length: 224.254464±157.600093, min=28.000000, max=3086.000000, size=22459<br></code></pre></td></tr></table></figure><h2 id="编写微调脚本"><a href="#编写微调脚本" class="headerlink" title="编写微调脚本"></a>编写微调脚本</h2><p>SWIFT框架提供了部分大模型的微调脚本，可以在我们下载的源码中的<em>swift&#x2F;examples&#x2F;pytorch&#x2F;llm&#x2F;scripts</em>路径中找到这些脚本。如果这些脚本能够满足我们大部分的微调需求，我们可以选择直接对这些脚本进行修改。如果找不到我们需要的脚本，需要我们根据<em>swift&#x2F;docs&#x2F;source&#x2F;LLM</em>中的命令行参数文档自行编写训练脚本。</p><p>以下是对Qwen1.5-14B-Chat进行全参微调的一个训练脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">Experimental environment: 8 * A100 40GB</span><br>nproc_per_node=1<br>NPROC_PER_NODE=$nproc_per_node \<br>MASTER_PORT=29500 \<br>CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \<br>swift sft \<br>    --model_type qwen1half-14b-chat \<br>    --model_id_or_path /yldm0226/models/Qwen1.5-14B-Chat \<br>    --model_revision master \<br>    --sft_type full \<br>    --tuner_backend swift \<br>    --template_type AUTO \<br>    --dtype AUTO \<br>    --output_dir /yldm0226/llm_sft_output \<br>    --ddp_backend nccl \<br>    --custom_train_dataset_path /yldm0226/data/1-Chinese_medical_dialogue_six_department.jsonl /yldm0226/data/2-HuatuoGPT2_sft_instruct_GPT4.jsonl /yldm0226/data/3-ChatMed_Consult-v0.3.jsonl /yldm0226/data/4-ChatMed_TCM-v0.2.jsonl /yldm0226/data/5-QiZhen_sft_20k.jsonl /yldm0226/data/6-Huatuo_Lite.jsonl /yldm0226/data/7-ZhongJing_CMtMedQA.jsonl /yldm0226/data/8-DISC-Med-SFT_released.jsonl /yldm0226/data/9-SZY_TCM_QA.jsonl \<br>    --train_dataset_sample -1 \<br>    --num_train_epochs 1 \<br>    --max_length 4096 \<br>    --check_dataset_strategy warning \<br>    --gradient_checkpointing true \<br>    --batch_size 1 \<br>    --weight_decay 0.01 \<br>    --learning_rate 1e-4 \<br>    --gradient_accumulation_steps $(expr 8 / $nproc_per_node) \<br>    --max_grad_norm 0.5 \<br>    --warmup_ratio 0.03 \<br>    --eval_steps 100 \<br>    --save_steps 100 \<br>    --save_total_limit 3 \<br>    --logging_steps 10 \<br>    --use_flash_attn false \<br>    --save_only_model true \<br></code></pre></td></tr></table></figure><p>下面对该脚本中的一些重要参数作出解释：</p><ul><li><code>nproc_per_node</code>：在多机分布式训练中，每个主机被当做一个node；nproc_per_node代表的是每个node中有几个线程；以该脚本为例，该脚本运行在单机环境中，因此nproc_per_node就代表着我们使用的单台服务器有几个线程去同时训练模型；假如nproc_per_node设置为8，那么将有8个线程同时训练模型，速度会提高很多，但是这样每块GPU都要负责存储完整的模型权重，显存会受到很大的挑战；假如nproc_per_node设置为4，那么将有4个线程同时训练模型，每个线程中有两块GPU，这两块GPU共同负责存储模型权重，这样虽然速度降低了，但是能够得到更宽裕的GPU显存；但此时我们的显存还是不足以训练14B的Qwen1.5-Chat，我们只能舍弃时间以换取空间，所以将nproc_per_node设置为1，此时该服务器只有1个线程去训练模型，模型被切分到8块GPU卡上。请注意，模型被切分到8块GPU卡上时，并不代表着这8块GPU只需要承担模型部分权重的显存，还需要承担优化器中各种梯度的存储，这也是相当大的一部分显存开销。所以如果显存允许，我们应该尽可能的提高nproc_per_node的值，提高显卡的利用率。</li><li><code>CUDA_VISIBLE_DEVICES</code>：服务器中如果有多张显卡，可以通过该参数指定使用哪几张显卡。显卡的序号可以通过<em>nvidia-smi</em>查看。</li><li><code>model_type</code>：model_type指定我们要微调的大模型的类型，这些类型必须是SWIFT框架所支持大模型类型的一种，具体有哪些支持的模型可以在swift源码的<em>swift&#x2F;docs&#x2F;source&#x2F;LLM</em>路径中的支持的模型和数据集文档中查看。</li><li><code>model_id_or_path</code>：model_id_or_path用于指定大模型权重的本地路径。</li><li><code>sft_type</code>: sft_type表示微调的方式, 默认是lora。可以选择的值包括: ‘lora’, ‘full’, ‘longlora’, ‘qalora’。此处使用的是full，即全参微调。</li><li><code>output_dir</code>：output_dir用于指定大模型微调过程中输出日志的存储路径。</li><li><code>custom_train_dataset_path</code>：用于指定我们数据集的存放路径，每个数据集之间用空格分隔。</li><li><code>train_dataset_sample</code>：对训练集进行采样, 默认是20000, 用于加快训练的速度。 该参数是为了避免数据集过大, 单个epoch训练时间过长的问题。 如果你指定为-1, 则使用完整的训练集进行训练。</li><li><code>max_length</code>：token的最大长度, 默认为2048。该参数可以避免个别过长的数据样本造成OOM的问题。当指定–truncation_strategy delete时, 如果某数据样本长度超过max_length, 我们会删除该数据样本。如果指定–truncation_strategy truncation_left时, 我们会切除最前面的token: input_ids[-max_length:]。如果设置为-1, 则无限制。该参数很重要，要根据显存情况选择合适的max_length，不然在训练中会出现OOM的情况，导致训练终止。</li></ul><p>对于其他参数，这里不做过多讲解。此外，这个脚本只涉及到了部分参数，如果需要进一步的定制化，需根据文档自行修改。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>运行脚本，可以得到以下信息：</p><p>数据集预处理所需要的时间大概30分钟：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">703283/2223540 [09:23&lt;20:31, 1234.07it/s] <br></code></pre></td></tr></table></figure><p>max_length设置为2048的情况下，估算训练时间：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">100/277856 [05:38&lt;255:02:02,  3.31s/it]<br><br>8596/22452 [10:15&lt;17:42, 13.04it/s]<br></code></pre></td></tr></table></figure><p>训练一个epoch大约需要255小时；进行一次验证大约需要27分钟，我们设置每100步进行一次验证，总步数为277856，需要进行2778次验证，预计用时为1250小时；当然，设置100步进行一次验证有些太频繁了，在实际进行训练时可以设置为10000步进行一次验证，预计用时为12.6小时。(这里的时间只是一个大概值，在训练时，不同数据的处理速度不同，花费的总时间会一直变化)</p><p>max_length设置为4096的情况下，训练所需的大概时间如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">100/277926 [05:50&lt;259:11:42,  3.36s/it]<br><br>1512/22459 [01:43&lt;24:31, 14.24it/s]<br></code></pre></td></tr></table></figure><p>情况与max_length设置为2048的情况差距不大。</p><p>max_length设置为8192的情况下，显存不够了，出现溢出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.95 GiB (GPU 0; 39.45 GiB total capacity; 26.79 GiB already allocated; 8.68 GiB free; 29.46 GiB reserved <span class="hljs-keyword">in</span> total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation <span class="hljs-keyword">for</span> Memory Management and PYTORCH_CUDA_ALLOC_CONF<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/09/23/2-%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90(Retrieval-augmented%20Generation,RAG)%E5%AE%9E%E6%88%981-%E5%9F%BA%E4%BA%8ELlamaIndex%E6%9E%84%E5%BB%BA%E7%AC%AC%E4%B8%80%E4%B8%AARAG%E5%BA%94%E7%94%A8/"/>
    <url>/2024/09/23/2-%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90(Retrieval-augmented%20Generation,RAG)%E5%AE%9E%E6%88%981-%E5%9F%BA%E4%BA%8ELlamaIndex%E6%9E%84%E5%BB%BA%E7%AC%AC%E4%B8%80%E4%B8%AARAG%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="检索增强生成-Retrieval-augmented-Generation-RAG-实战1-基于LlamaIndex构建第一个RAG应用"><a href="#检索增强生成-Retrieval-augmented-Generation-RAG-实战1-基于LlamaIndex构建第一个RAG应用" class="headerlink" title="检索增强生成(Retrieval-augmented Generation,RAG)实战1-基于LlamaIndex构建第一个RAG应用"></a>检索增强生成(Retrieval-augmented Generation,RAG)实战1-基于LlamaIndex构建第一个RAG应用</h1><p>本文将介绍如何使用LlamaIndex构建一个非常简单的RAG应用。通过该案例，可以初步了解LlamaIndex构建RAG应用的大体流程。</p><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><h3 id="安装LlamaIndex"><a href="#安装LlamaIndex" class="headerlink" title="安装LlamaIndex"></a>安装LlamaIndex</h3><p>安装LlamaIndex相关包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install llama-index<br>pip install llama-index-embeddings-huggingface<br>pip install llama-index-llms-huggingface<br></code></pre></td></tr></table></figure><p>llama-index是核心包；llama-index-embeddings-huggingface允许我们使用本地的embedding模型去完成文档的切分和编码等操作；llama-index-llms-huggingface允许我们使用本地的大模型去开发RAG应用。</p><p>安装完成后，检查服务器cuda版本是否与当前安装的pytorch对应，如果不对应，需要将pytorch版本降低到≤服务器cuda版本（详细见</p><p>）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2<br></code></pre></td></tr></table></figure><h3 id="下载embedding模型权重"><a href="#下载embedding模型权重" class="headerlink" title="下载embedding模型权重"></a>下载embedding模型权重</h3><p>使用BAAI开源的中文bge模型作为embedding模型，使用以下命令将模型权重下载到服务器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://huggingface.co/BAAI/bge-base-zh-v1.5<br></code></pre></td></tr></table></figure><h3 id="下载大模型权重"><a href="#下载大模型权重" class="headerlink" title="下载大模型权重"></a>下载大模型权重</h3><p>使用阿里开源的通义千问大模型，使用以下命令将模型权重下载到服务器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://www.modelscope.cn/qwen/Qwen1.5-14B-Chat.git<br></code></pre></td></tr></table></figure><h2 id="构建第一个RAG应用-中医临床诊疗术语证候问答"><a href="#构建第一个RAG应用-中医临床诊疗术语证候问答" class="headerlink" title="构建第一个RAG应用-中医临床诊疗术语证候问答"></a>构建第一个RAG应用-中医临床诊疗术语证候问答</h2><h3 id="文档准备"><a href="#文档准备" class="headerlink" title="文档准备"></a>文档准备</h3><p>本应用使用的文档是由国家卫生健康委员和会国家中医药管理局发布的<strong>中医临床诊疗术语 第2部分：证候</strong>。其部分内容展示如下：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-number">3.5.5.5</span><br>    湿浊蒙窍证  syndrome/pattern of dampness-turbidity clouding orifices<br>    因湿浊壅盛，上蒙清窍所致。临床以头重闷胀，眩晕欲仆，恶心，呕吐唾沫，胸闷，舌苔白厚或垢腻，脉濡缓或滑，可伴见脑鸣、耳胀，听音不真，或眼球震颤，视物模糊，眼前有灰黄色暗影遮挡，或鼻塞、涕浊，不闻香臭等为特征的证候。<br><br><span class="hljs-number">3.5.5.6</span><br>    湿浊上泛证  syndrome/pattern of dampness-turbidity flooding in the upper<br>    湿浊蒙上，泌别失职证<br>    湿浊蒙上证<br>    因湿浊内蕴，邪犯清空，泌别失职所致。临床以头晕作胀，神志昏蒙、恍惚，恶心、呕吐，面色晦滞，少尿或无尿，舌质淡，舌苔厚浊，脉沉缓，可伴见脘腹闷胀，不思饮食，皮肤干燥、瘙痒等为特征的证候。<br><br><span class="hljs-number">3.5.5.7</span><br>    湿浊冲心证  syndrome/pattern of dampness-turbidity attacking heart<br>    因湿浊内蕴，壅阻心脉，上攻冲心，壅闭心神所致。临床以胸膺憋闷，心悸、怔忡，或神志恍惚，甚则昏昧，言语时或错乱，舌苔厚腻，脉弦或缓，可伴见心胸痹痛，面色晦滞，呕吐、不食等为特征的证候。<br></code></pre></td></tr></table></figure><p>将<em>中医临床诊疗术语证候.txt</em>放入项目根目录下的<em>document</em>文件夹中。</p><h3 id="导入所需的包"><a href="#导入所需的包" class="headerlink" title="导入所需的包"></a>导入所需的包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">import</span> sys<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> PromptTemplate, Settings, SimpleDirectoryReader, VectorStoreIndex, load_index_from_storage, \<br>    StorageContext, QueryBundle<br><span class="hljs-keyword">from</span> llama_index.core.schema <span class="hljs-keyword">import</span> MetadataMode<br><span class="hljs-keyword">from</span> llama_index.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbedding<br><span class="hljs-keyword">from</span> llama_index.llms.huggingface <span class="hljs-keyword">import</span> HuggingFaceLLM<br><span class="hljs-keyword">from</span> llama_index.core.node_parser <span class="hljs-keyword">import</span> SentenceSplitter<br></code></pre></td></tr></table></figure><h3 id="定义日志配置"><a href="#定义日志配置" class="headerlink" title="定义日志配置"></a>定义日志配置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">logging.basicConfig(stream=sys.stdout, level=logging.INFO)<br>logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))<br></code></pre></td></tr></table></figure><p>如果想看到更多的日志输出，将level的级别改为logging.DEBUG。</p><h3 id="定义System-Prompt"><a href="#定义System-Prompt" class="headerlink" title="定义System Prompt"></a>定义System Prompt</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">SYSTEM_PROMPT = <span class="hljs-string">&quot;&quot;&quot;You are a helpful AI assistant.&quot;&quot;&quot;</span><br>query_wrapper_prompt = PromptTemplate(<br>    <span class="hljs-string">&quot;[INST]&lt;&lt;SYS&gt;&gt;\n&quot;</span> + SYSTEM_PROMPT + <span class="hljs-string">&quot;&lt;&lt;/SYS&gt;&gt;\n\n&#123;query_str&#125;[/INST] &quot;</span><br>)<br></code></pre></td></tr></table></figure><h3 id="使用llama-index-llms-huggingface构建本地大模型"><a href="#使用llama-index-llms-huggingface构建本地大模型" class="headerlink" title="使用llama-index-llms-huggingface构建本地大模型"></a>使用llama-index-llms-huggingface构建本地大模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">llm = HuggingFaceLLM(<br>    context_window=<span class="hljs-number">4096</span>,<br>    max_new_tokens=<span class="hljs-number">2048</span>,<br>    generate_kwargs=&#123;<span class="hljs-string">&quot;temperature&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>&#125;,<br>    query_wrapper_prompt=query_wrapper_prompt,<br>    tokenizer_name=<span class="hljs-string">&#x27;/yldm0226/models/Qwen1.5-14B-Chat&#x27;</span>,<br>    model_name=<span class="hljs-string">&#x27;/yldm0226/models/Qwen1.5-14B-Chat&#x27;</span>,<br>    device_map=<span class="hljs-string">&quot;auto&quot;</span>,<br>    model_kwargs=&#123;<span class="hljs-string">&quot;torch_dtype&quot;</span>: torch.float16&#125;,<br>)<br>Settings.llm = llm<br></code></pre></td></tr></table></figure><p><em>tokenizer_name</em>和<em>model_name</em>后面的路径为前面下载的大模型权重的存储路径。</p><h3 id="使用llama-index-embeddings-huggingface构建本地embedding模型"><a href="#使用llama-index-embeddings-huggingface构建本地embedding模型" class="headerlink" title="使用llama-index-embeddings-huggingface构建本地embedding模型"></a>使用llama-index-embeddings-huggingface构建本地embedding模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">Settings.embed_model = HuggingFaceEmbedding(<br>    model_name=<span class="hljs-string">&quot;/yldm0226/RAG/BAAI/bge-base-zh-v1.5&quot;</span><br>)<br></code></pre></td></tr></table></figure><p>model_name是前面下载的embedding模型权重的存储路径。</p><h3 id="读取文档"><a href="#读取文档" class="headerlink" title="读取文档"></a>读取文档</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">documents = SimpleDirectoryReader(<span class="hljs-string">&quot;document&quot;</span>).load_data()<br></code></pre></td></tr></table></figure><h3 id="对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引"><a href="#对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引" class="headerlink" title="对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引"></a>对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">index = VectorStoreIndex.from_documents(documents, transformations=[SentenceSplitter(chunk_size=<span class="hljs-number">256</span>)])<br></code></pre></td></tr></table></figure><p><em>chunk_size</em>可以控制切分片段的大小。</p><h3 id="构建查询引擎"><a href="#构建查询引擎" class="headerlink" title="构建查询引擎"></a>构建查询引擎</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">query_engine = index.as_query_engine(similarity_top_k=<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><p><em>similarity_top_k</em>决定了我们将检索出多少个片段用于RAG。</p><h3 id="获得答案"><a href="#获得答案" class="headerlink" title="获得答案"></a>获得答案</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">response = query_engine.query(<span class="hljs-string">&quot;不耐疲劳，口燥、咽干可能是哪些证候？&quot;</span>)<br></code></pre></td></tr></table></figure><p>稍作等待后，得到以下输出：</p><p>从中医的角度来看，口燥、咽干且不耐疲劳可能与以下几个证候相关：</p><ol><li><p><strong>津液不足证</strong> 或 <strong>津亏证</strong>：由于津液生成不足或者体内燥热导致津液耗损，表现为口眼喉鼻干燥，大便干结，小便少，舌质红干，脉细数。</p></li><li><p><strong>津亏热结证</strong>：津液亏乏加上热邪内结，也可见口燥咽干，可能伴有便秘。</p></li><li><p><strong>津液亏耗证</strong> 或 <strong>液干热结证</strong>：津液亏损可能导致口干、口渴，皮肤干燥，甚至影响消化功能（如大便干结）。</p></li><li><p><strong>津液亏涸证</strong>：严重的津液亏损会出现口唇干燥、鼻燥、皮肤干瘪等症状。</p></li><li><p><strong>燥干清窍证</strong>：长期气候干燥或环境因素导致的津液耗损，表现为口鼻咽喉干燥，眼睛干涩。</p></li><li><p><strong>津伤化燥证</strong> 或 <strong>津伤燥热证</strong>：体内燥热伤津，也会有口干、舌燥、食多善饥、皮肤干燥等表现。</p></li></ol><p>综合以上，这些证候都有可能涉及到口燥咽干和不耐疲劳的症状，但具体需要结合病史和全身症状来确诊。建议就诊中医师进行辨证论治。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>搭建一个大模型API服务</title>
    <link href="/2024/03/01/1-%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%A4%A7%E6%A8%A1%E5%9E%8BAPI%E6%9C%8D%E5%8A%A1/"/>
    <url>/2024/03/01/1-%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%A4%A7%E6%A8%A1%E5%9E%8BAPI%E6%9C%8D%E5%8A%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="搭建一个大模型API服务"><a href="#搭建一个大模型API服务" class="headerlink" title="搭建一个大模型API服务"></a>搭建一个大模型API服务</h1><p>本文将介绍如何使用SWIFT框架搭建一个大模型API服务，以方便后续做RAG、Agent的开发工作。</p><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>基础环境</p><ul><li>操作系统：Ubuntu 18.04.5 LTS (GNU&#x2F;Linux 3.10.0-1127.el7.x86_64 x86_64)</li><li>Anaconda3：Anaconda3-2023.03-1-Linux-x86_64</li><li>根据服务器网络情况配置好conda源和pip源，此处使用的是超算山河源</li></ul><p>创建一个新的conda环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda create --name swift python=3.8<br></code></pre></td></tr></table></figure><p>激活刚刚创建的conda环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda activate swift<br></code></pre></td></tr></table></figure><p>下载SWIFT源码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/modelscope/swift.git<br></code></pre></td></tr></table></figure><p>切换到SWIFT路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /yldm0226/swift<br></code></pre></td></tr></table></figure><p>安装SWIFT：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install -e .[llm]<br></code></pre></td></tr></table></figure><p>检查服务器cuda版本是否与当前安装的pytorch对应，如果不对应，需要将pytorch版本降低到≤服务器cuda版本；使用nvidia-smi查看cuda版本：</p><p><img src="/../images/nvidia-smi.png"></p><p>当前cuda版本为11.7; 然后使用conda list检查swift环境中pytorch的版本：</p><p><img src="/../images/pytorch%E7%89%88%E6%9C%AC.png"></p><p>pytorch版本为2.2.0，从官网查询可知2.2.0版本最低支持的cuda版本为11.8，这大于服务器cuda版本11.7，因此需要将pytorch降低到支持cuda11.7的版本，从官网查询可知目前最高的可支持cuda11.7的版本为2.0.1。执行下述命令更换pytorch版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2<br></code></pre></td></tr></table></figure><p>注：如果pytorch版本与服务器cuda版本不对应，程序不会报错，而是会输出下面的警告，此时SWIFT会将模型权重加载到CPU上。推理速度非常慢：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)<br></code></pre></td></tr></table></figure><p>将大模型权重下载到本地，可以在</p><p>或</p><p>选择对应模型下载，这里以Qwen1.5-14B-Chat为例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://www.modelscope.cn/qwen/Qwen1.5-14B-Chat.git<br></code></pre></td></tr></table></figure><h2 id="搭建API服务"><a href="#搭建API服务" class="headerlink" title="搭建API服务"></a>搭建API服务</h2><h3 id="启动服务端"><a href="#启动服务端" class="headerlink" title="启动服务端"></a>启动服务端</h3><h4 id="单卡部署"><a href="#单卡部署" class="headerlink" title="单卡部署"></a>单卡部署</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">CUDA_VISIBLE_DEVICES=0 swift deploy --model_type qwen1half-14b-chat --model_id_or_path /yldm0226/models/Qwen1.5-14B-Chat<br></code></pre></td></tr></table></figure><p>CUDA_VISIBLE_DEVICES可以指定使用哪块GPU进行部署；model_type表示你选择的模型类型，类型需是SWIFT框架支持的模型种类的一种，可以在</p><p>查询所有支持的模型；–model_id_or_path表示模型在ModelScope Hub中的<code>model_id</code>或者本地路径，当指定本地路径时，代码会优先加载本地路径中的模型权重。</p><p>部署成功后，可以看到以下输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">INFO:     Started server process [82478]<br>INFO:     Waiting <span class="hljs-keyword">for</span> application startup.<br>INFO:     Application startup complete.<br>INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)<br></code></pre></td></tr></table></figure><h4 id="多卡部署"><a href="#多卡部署" class="headerlink" title="多卡部署"></a>多卡部署</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">RAY_memory_monitor_refresh_ms=0 CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 swift deploy --model_type qwen1half-14b-chat --model_id_or_path /yldm0226/models/Qwen1.5-14B-Chat --tensor_parallel_size 8<br></code></pre></td></tr></table></figure><p>同样的，在部署成功后可以看到以下输出：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">INFO</span>:     Started <span class="hljs-keyword">server</span> process [<span class="hljs-number">100001</span>]<br><span class="hljs-keyword">INFO</span>:     Waiting <span class="hljs-keyword">for</span> application startup.<br><span class="hljs-keyword">INFO</span>:     Application startup complete.<br><span class="hljs-keyword">INFO</span>:     Uvicorn running <span class="hljs-keyword">on</span> http://<span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">8000</span> (Press CTRL+C <span class="hljs-keyword">to</span> quit)<br></code></pre></td></tr></table></figure><p>可以通过nvidia-smi命令查看多卡部署的显存占用情况：</p><p><img src="/../images/%E5%A4%9A%E5%8D%A1%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8.png"></p><h3 id="客户端测试"><a href="#客户端测试" class="headerlink" title="客户端测试"></a>客户端测试</h3><h4 id="curl"><a href="#curl" class="headerlink" title="curl"></a>curl</h4><p>想要快速测试API的可用性，可以使用curl：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl http://localhost:8000/v1/chat/completions \<br>-H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> \<br>-d <span class="hljs-string">&#x27;&#123;</span><br><span class="hljs-string">&quot;model&quot;: &quot;qwen1half-14b-chat&quot;,</span><br><span class="hljs-string">&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请介绍一下API服务&quot;&#125;],</span><br><span class="hljs-string">&quot;max_tokens&quot;: 1024,</span><br><span class="hljs-string">&quot;temperature&quot;: 0</span><br><span class="hljs-string">&#125;&#x27;</span><br></code></pre></td></tr></table></figure><p>model与部署时的model_type一致。</p><p>可以得到下面形式的响应：</p><figure class="highlight wren"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs wren">&#123;<span class="hljs-string">&quot;model&quot;</span>:<span class="hljs-string">&quot;qwen1half-14b-chat&quot;</span>,<span class="hljs-string">&quot;choices&quot;</span>:[&#123;<span class="hljs-string">&quot;index&quot;</span>:<span class="hljs-number">0</span>,<span class="hljs-string">&quot;message&quot;</span>:&#123;<span class="hljs-string">&quot;role&quot;</span>:<span class="hljs-string">&quot;assistant&quot;</span>,<span class="hljs-string">&quot;content&quot;</span>:<span class="hljs-string">&quot;API（Application Programming Interface，应用程序编程接口）服务是一种接口，它允许不同的软件应用程序之间进行交互和数据共享。API是一组预定义的规则、协议和工具，开发者可以使用它来调用或访问某个应用程序、平台或服务的功能，而无需了解其底层实现细节。简单来说，它就像一个“桥梁”，使得开发者能够编写自己的程序，通过这个接口来获取数据、执行操作或者触发特定功能。<span class="hljs-char escape_">\n</span><span class="hljs-char escape_">\n</span>API服务通常分为以下几种类型：<span class="hljs-char escape_">\n</span><span class="hljs-char escape_">\n</span>1. **Web API**：基于HTTP协议，用于Web应用间的通信，如RESTful API，它以HTTP请求（GET、POST、PUT、DELETE等）的形式发送数据。<span class="hljs-char escape_">\n</span><span class="hljs-char escape_">\n</span>2. **SDK API**：软件开发工具包（Software Development Kit）中的API，提供了特定平台或服务的编程接口，如Google Maps API、Facebook API等。<span class="hljs-char escape_">\n</span><span class="hljs-char escape_">\n</span>3. **企业级API**：企业内部或外部提供的API，用于内部系统集成，如CRM系统API、支付API等。<span class="hljs-char escape_">\n</span><span class="hljs-char escape_">\n</span>4. **机器学习/AI API**：如Google Cloud的机器学习API，允许开发者使用预训练模型进行预测或处理任务。<span class="hljs-char escape_">\n</span><span class="hljs-char escape_">\n</span>5. **API Gateway**：一种服务，它集中管理多个API，提供安全、路由、缓存等功能，如AWS API Gateway。<span class="hljs-char escape_">\n</span><span class="hljs-char escape_">\n</span>通过API服务，开发者可以快速地扩展功能、集成第三方服务，提高开发效率，同时促进了软件生态系统的繁荣。&quot;</span>&#125;,<span class="hljs-string">&quot;finish_reason&quot;</span>:<span class="hljs-literal">null</span>&#125;],<span class="hljs-string">&quot;usage&quot;</span>:&#123;<span class="hljs-string">&quot;prompt_tokens&quot;</span>:<span class="hljs-number">23</span>,<span class="hljs-string">&quot;completion_tokens&quot;</span>:<span class="hljs-number">294</span>,<span class="hljs-string">&quot;total_tokens&quot;</span>:<span class="hljs-number">317</span>&#125;,<span class="hljs-string">&quot;id&quot;</span>:<span class="hljs-string">&quot;chatcmpl-f7fa52fbf7de45f1bc1a31e369482a19&quot;</span>,<span class="hljs-string">&quot;object&quot;</span>:<span class="hljs-string">&quot;chat.completion&quot;</span>,<span class="hljs-string">&quot;created&quot;</span>:<span class="hljs-number">1709258739</span>&#125;<br></code></pre></td></tr></table></figure><h4 id="swift"><a href="#swift" class="headerlink" title="swift"></a>swift</h4><p>也可以使用swift框架去编写客户端代码，下面是一个简单的示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> swift.llm <span class="hljs-keyword">import</span> get_model_list_client, XRequestConfig, inference_client<br><br>model_list = get_model_list_client()<br>model_type = model_list.data[<span class="hljs-number">0</span>].<span class="hljs-built_in">id</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;model_type: <span class="hljs-subst">&#123;model_type&#125;</span>&#x27;</span>)<br><br><span class="hljs-comment"># 直接输出</span><br>query = <span class="hljs-string">&#x27;山东的省会在哪里?&#x27;</span><br>request_config = XRequestConfig(seed=<span class="hljs-number">42</span>)<br>resp = inference_client(model_type, query, request_config=request_config)<br>response = resp.choices[<span class="hljs-number">0</span>].message.content<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;query: <span class="hljs-subst">&#123;query&#125;</span>&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;response: <span class="hljs-subst">&#123;response&#125;</span>&#x27;</span>)<br><br><span class="hljs-comment"># 流式输出</span><br>history = [(query, response)]<br>query = <span class="hljs-string">&#x27;这有什么好吃的?&#x27;</span><br>request_config = XRequestConfig(stream=<span class="hljs-literal">True</span>, seed=<span class="hljs-number">42</span>)<br>stream_resp = inference_client(model_type, query, history, request_config=request_config)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;query: <span class="hljs-subst">&#123;query&#125;</span>&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;response: &#x27;</span>, end=<span class="hljs-string">&#x27;&#x27;</span>)<br><span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> stream_resp:<br>    <span class="hljs-built_in">print</span>(chunk.choices[<span class="hljs-number">0</span>].delta.content, end=<span class="hljs-string">&#x27;&#x27;</span>, flush=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>()<br></code></pre></td></tr></table></figure><p>第一个问题’山东的省会在哪里?’使用的是直接返回response的方式，第二个问题’这有什么好吃的?’使用的是流式输出的方式，可以根据需要选择对应的方式。</p><p>运行程序，可以得到以下输出：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs markdown">query: 山东的省会在哪里?<br>response: 山东省的省会是济南。<br>query: 这有什么好吃的?<br>response: 山东作为中国的一个大省，美食丰富多样，这里有许多著名的特色菜肴和小吃。以下是一些你可能会感兴趣的：<br><br><span class="hljs-bullet">1.</span> 热干面（不是山东本地的，但济南也有人喜欢）：源于武汉，但在山东也有类似面食。<br><span class="hljs-bullet">2.</span> 鲁菜：山东菜系，以济南菜为代表，如糖醋黄河鲤鱼、九转大肠、葱烧海参、锅包肉等，口味偏重，讲究原汁原味。<br><span class="hljs-bullet">3.</span> 烤鸭：虽然以北京最有名，但山东济南的烤鸭店也有特色。<br><span class="hljs-bullet">4.</span> 鲅鱼水饺：山东沿海城市如青岛的特色，用新鲜鲅鱼做馅，鲜美可口。<br><span class="hljs-bullet">5.</span> 豆腐脑：搭配薄饼、香菜、榨菜，是早餐的常见选择。<br><span class="hljs-bullet">6.</span> 煎饼：山东大煎饼，尤其是临沂煎饼，薄脆可卷各种食材。<br><span class="hljs-bullet">7.</span> 油旋：济南特色小吃，类似油条但更细，酥脆可口。<br><br>这只是冰山一角，山东各地还有许多其他美食，如鲁西南驴肉火烧、莱阳梨、胶东海鲜等，去山东旅行不妨尝试一下。<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大语言模型入门教程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型, API</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
